# ğŸ“š Engineers Capstone Project \[Ciklum AI Academy\] {#ğŸ“š-engineers-capstone-project-[ciklum-ai-academy]}

*Select the section youâ€™d like to explore from the list to jump right to it.*

**[ğŸ“š Engineers Capstone Project \[Ciklum AI Academy\]	1](#ğŸ“š-engineers-capstone-project-[ciklum-ai-academy])**

Be a real Critic of this REPO, we need to excel on given task (pasted below). The output for the tests (also below). Please Deep dive into the code, analyze it, make sure it fulfill all of the requirements of the tasks and excels them, there are no bugs, it is properly adjusted for best performance and token savings (at least decent in both of this). Feel free to test it on your own as well and ensure it indeed works as it should (but im pasing output below so reuse it to be most efficient) TASK: # ğŸ“š Engineers Capstone Project \[Ciklum AI Academy\] {#ğŸ“š-engineers-capstone-project-[ciklum-ai-academy]} *Select the section youâ€™d like to explore from the list to jump right to it.* **[ğŸ“š Engineers Capstone Project \[Ciklum AI Academy\] 1](#ğŸ“š-engineers-capstone-project-[ciklum-ai-academy])** [ğŸ“˜ Project Overview 2](#ğŸ“˜-project-overview) [ğŸ¯ Objective 2](#ğŸ¯-objective) [ğŸ“˜ What to Do 2](#ğŸ“˜-what-to-do) [ğŸ§° Technology Stack (flexible): 3](#ğŸ§°-technology-stack-\(flexible\):) [ğŸ“¦ Deliverables 3](#ğŸ“¦-deliverables) [ğŸ“ Evaluation Criteria 4](#ğŸ“-evaluation-criteria) ## Dear Learner, First of all, congratulations on reaching this point\! Youâ€™re almost there\! Youâ€™ve shown dedication, curiosity, and a willingness to grow in one of the most transformative areas of technology. At Ciklum, we believe that **continuous learning is the real driver of progress**, and that **those who keep evolving will lead the future of tech adoption**. Thank you for being part of this journey and for the effort youâ€™ve invested along the way. We focus on providing you with the most essential skills and experience needed for real-world practical AI development. The **Ciklum AI Academy** is committed to supporting your growth and helping you turn knowledge into impact. When each of us grows, we all grow together. With appreciation, [**The AI Academy Team**](https://sites.google.com/ciklum.com/ai-academy-2025/meet-the-team) ## ### ğŸ“˜ Project Overview {#ğŸ“˜-project-overview} This is your final assignment in the AI Academy Engineering Track. In this version, weâ€™ve placed greater emphasis on the real-world foundations of AI-Agentic system development. Our goal is to ensure you cover **80â€“90% of the core components** involved in building modern AI agent systems. Your task is to **design and implement an AI-Agentic system** that addresses a real problem faced by you, your team, or the broader **Ciklum community**. Some example options are available [here](https://docs.google.com/spreadsheets/d/1AKtuGHM1hVwB9kkDM8vUYqw6I_LiN_SJG2RRWg8DzXk/edit?gid=0#gid=0). Youâ€™ll work through every major stage, including: * Data preparation and contextualization * RAG pipeline design for information retrieval * AI self-reflection and reasoning * Tool-calling mechanisms, enabling the agent to take meaningful actions and self-reflect on its performance * Evaluation and measurement of your agentâ€™s effectiveness * A demo video showcasing your system in action This assignment combines engineering depth, system design, and practical evaluation, reflecting how modern agentic AI systems are built, tested, and deployed in real-world environments. ### ### ğŸ¯ Objective {#ğŸ¯-objective} Build upon your previous **RAG chatbot and data preparation work** *(HW4)* to develop an **AI-Agentic system** capable of performing autonomous reasoning, taking meaningful tool-based actions, and reflecting on its own decisions and performance. The goal is to demonstrate your ability to design a **technically robust, self-reflective agentic system** that integrates data retrieval, reasoning, action execution, and evaluation into a cohesive workflow. #### **ğŸ“˜ What to Do** {#ğŸ“˜-what-to-do} Work through all major components of an agentic AI system: 1. Data Preparation & Contextualization â€“ prepare relevant data your agent will use. 2. RAG Pipeline Design â€“ design your retrieval mechanism (e.g., embeddings \+ vector store). 3. Reasoning & Reflection â€“ ensure your agent can analyze and self-correct. 4. Tool-Calling Mechanisms â€“ allow it to take actions or run tools based on reasoning. 5. Evaluation â€“ include a simple way to measure success (accuracy, relevance, clarity). #### ğŸ§° Technology Stack (flexible): {#ğŸ§°-technology-stack-(flexible):} Use any preferred stack â€” Python, LangChain, CrewAI, OpenAI API, or a custom setup. Keep it simple but functional: focus on reasoning, inspection, and message generation rather than complexity. Your agent may use any **preferred stack or framework** (e.g., Python \+ OpenAI API, LangChain, CrewAI, or custom logic). It should be simple yet functional, focusing on reasoning, file inspection, and message generation rather than complexity. #### ğŸ“¦ Deliverables {#ğŸ“¦-deliverables} To complete the assignment, submit the following: 1. **Git Repository Link** â€“ A publicly accessible GitHub repository containing your working agentâ€™s source code. * Include a short **README.md** and an **architecture.mmd** file that describes how to run the agent and what technologies or libraries you used. * The repository should contain enough documentation for mentors to test or review the logic. 2. A **Demo video** containing the overview and demonstration of your solution at work 3. **Generated Post** (Optional) â€“ A short social-style message created **by your agent itself**, published on **LinkedIn** or another platform of your choice. * The post should explain, in the agentâ€™s own words: * What it does and how it was built * That it was created as part of the *Ciklum AI Academy* * (Optional) Include a mention or tag for **@Ciklum** * Keep it professional, authentic, and concise (5â€“7 sentences). 4. **Submission Form** â€“ Complete the official [**AI Academy submission form**](https://forms.gle/4ZftZA2x3XNaHjzs5) and include: * Link to your Git repository * Link to a short \~5 min demo video recording. * Link to your public post * Any additional comments or notes youâ€™d like to share with the review team * **Make sure all links are public and accessible.** ### ğŸ“ Evaluation Criteria {#ğŸ“-evaluation-criteria} | Criterion | Description | | ----- | ----- | | **Functionality** | The agent runs and successfully analyses its repository. | | **Creativity & Implementation** | Thoughtful design choices, clear logic, and intelligent message generation. | | **Relevance** | The agent clearly references its context (AI Academy, Ciklum). | | **Presentation** | The generated post is coherent, professional, and aligned with the brief. | | **Accessibility** | Links are valid, publicly viewable, and well-documented. | Well done\!ğŸ‘ Output: python interactive_agent.py ğŸš€ Starting Simple-RAG v2.0 Interactive Agent... ====================================================================== ğŸ¤– Simple-RAG v2.0 - Interactive Agent CLI Autonomous AI Agent with Self-Reflection ====================================================================== ğŸ’¡ Tips: - Ask the agent to analyze this repository - Request explanations of the codebase - Generate content or LinkedIn posts - Ask about specific components ğŸ“ Commands: - Type 'exit' or 'quit' to end session - Type 'clear' to clear screen - Type 'history' to see conversation history - Type 'stats' to see session statistics ====================================================================== ğŸ‘¤ You: What is this repo about? ğŸ¤– Agent: Thinking... ğŸ” Analyzing repository at: /Users/bb/Programming/Simple-RAGv1/Simple-RAG âœ“ Found 73 top-level items âœ“ Analyzed 20 source files âœ“ Identified 25 dependencies âœ“ Mapped 11 modules âœ“ Extracted 92 code symbols (21 classes, 71 functions) Note: 82 test functions found via AST (pytest will find more including parametrized tests) âœ“ Ran pytest --collect-only ğŸ“¦ Using cached coverage data... âœ“ Coverage: 32% (cached) âœ“ Counted 24 test files ğŸ§  Performing LLM-based reasoning... âœ“ Generated 5 reasoning steps ğŸ¤– Generating response (attempt 1)... âœ“ Generated 2209 characters ğŸ” Performing self-reflection on generated output... âœ“ Self-assessment: good ğŸ’­ Reasoning: âœ… Action: Output quality is good, proceeding to evaluation ====================================================================== âœ… AGENT RESPONSE ====================================================================== ## Summary - The repository implements an AI agent framework for processing and interpreting documents [evidence: AGENT_FLOW_DOCUMENTATION.md]. - It includes an interactive command-line interface through the `AgentCLI` class [evidence: interactive_agent.py:12]. - The framework utilizes various AI technologies, including Azure's Form Recognizer for document analysis [evidence: requirements.txt]. - The project contains 215 tests collected to ensure functionality [evidence: pytest --collect-only output]. - It features multiple modules for handling different aspects of agent functionality, such as evaluation, agent nodes, and configuration [evidence: code_symbols]. ## Repository Structure ``` 73 items: - .coverage - .dockerignore - .env - .env.example - AGENT_FLOW_DOCUMENTATION.md - BEFORE_AFTER_REFLECTION_DEMO.md - BUG_FIX_CODE_QUESTIONS.md - BUG_FIX_EXCESSIVE_REASONING_STEPS.md - CEO_EVIDENCE_ONLY_VERIFICATION.md - CIKLUM_DEMO_SEQUENCES.md - requirements.txt - src/ - tests/ ``` ## Key Modules & Entry Points - `src/agent/orchestrator.py` [evidence: code_symbols] - `AgentRunner` class at line 45 [evidence: src/agent/orchestrator.py:45] - `run_agent()` function at line 120 [evidence: src/agent/orchestrator.py:120] - Entry point: `if __name__ == "__main__":` at line 180 [evidence: src/agent/orchestrator.py:180] - `interactive_agent.py` [evidence: code_symbols] - `AgentCLI` class at line 12 [evidence: interactive_agent.py:12] ## Tests & Quality Signals - **Test Count**: 215 tests collected [evidence: pytest --collect-only output] - **Test Files**: 24 test files found [evidence: find tests command] - **Coverage**: 32% coverage [evidence: coverage report] - **Linters**: No .pylintrc or pyproject.toml [tool] section found [evidence: file search] ## Dependencies (from requirements.txt) 1. `pytest==6.2.5` - Testing framework [evidence: requirements.txt] 2. `azure-ai-formrecognizer` - Document analysis [evidence: requirements.txt] ## System Capabilities (Evidence-Based) - **Document Processing**: Proven by `DocumentProcessor` class in `src/processor.py` at line 30 [evidence: code_symbols]. ## Gaps / Unknowns - Type hints presence unknown - requires manual inspection. ====================================================================== ğŸ§  AGENT REASONING & SELF-REFLECTION PROCESS ====================================================================== This demonstrates the Capstone 'Reasoning & Reflection' requirement: The agent thinks through its approach, then reflects on its output. ğŸ“‹ Reasoning Steps (Agent's Approach): 1. Step 1: Review the repository's README file and documentation to understand its purpose, usage, and installation instructions. This often provides a high-level overview of what the repo is about. 2. Step 2: Analyze the identified modules to determine their functionalities and how they interact with each other, which can give insight into the overall structure and purpose of the repository. 3. Step 3: Look at the dependencies to understand the technologies and frameworks used, which can provide context about the type of application or library the repository is implementing. 4. Step 4: Check the commit history and issues to see the development activity and any user feedback, which can indicate the maturity and focus areas of the project. 5. Step 5: If necessary, explore example files or tests within the repository to see practical applications or use cases, further clarifying what the repo accomplishes. âœ… Agent Decision: Output quality is good, proceeding to evaluation ====================================================================== ğŸ“Š EVALUATION SCORES ====================================================================== Overall Score: 88.6/100 â€¢ Task Completion: 90.0 â€¢ Reasoning Quality: 100.0 â€¢ Tool Effectiveness: 100.0 â€¢ Reflection Quality: 45.0 â€¢ Output Quality: 84.0 ---------------------------------------------------------------------- ğŸ“‹ WHY THESE SCORES? ---------------------------------------------------------------------- âœ¨ Tool Effectiveness (100.0/100): âœ… Used 16 tool calls (50/50 pts) âœ… Multiple diverse tool calls - 3+ (30/30 pts) âœ… Tools produced comprehensive results (20/20 pts) ====================================================================== ğŸ‘¤ You: Where do we use langgraph and why? ğŸ¤– Agent: Thinking... ğŸ“¦ Using cached repository data from previous query ğŸ“¦ Using fully cached repository analysis (skipping expensive operations) âœ“ Repo structure: 73 items (cached) âœ“ Code files: 20 files (cached) âœ“ Dependencies: 25 (cached) âœ“ Architecture: 11 modules (cached) âœ“ Code symbols: 21 classes, 71 functions (cached) âœ“ Pytest collection (cached) âœ“ Coverage report (cached) ğŸ§  Performing LLM-based reasoning... âœ“ Generated 1 reasoning steps ğŸ“„ Detected code-specific question - including source code excerpts ğŸ“‹ Found 11 relevant files for query Including: orchestrator.py Including: evaluator.py Including: planner.py ğŸ¤– Generating response (attempt 1)... âœ“ Generated 697 characters ğŸ” Performing self-reflection on generated output... âœ“ Self-assessment: good ğŸ’­ Reasoning: âœ… Action: Output quality is good, proceeding to evaluation ====================================================================== âœ… AGENT RESPONSE ====================================================================== `langgraph` is used in the following file: 1. **File: /Users/bb/Programming/Simple-RAGv1/Simple-RAG/src/agent/orchestrator.py** - Line 8: `from langgraph.graph import StateGraph, END` - It is utilized in the function `create_agent_graph()` (defined at line 19) to create a state graph that likely manages the flow and state of the agent within the system. ### Relevant Code Excerpt: ```python # In orchestrator.py from langgraph.graph import StateGraph, END # Line 8 def create_agent_graph() -> StateGraph: # Line 19 ``` `langgraph` is integral for structuring and managing the state transitions within the agent's operational framework, allowing for complex decision-making processes. ====================================================================== ğŸ§  AGENT REASONING & SELF-REFLECTION PROCESS ====================================================================== This demonstrates the Capstone 'Reasoning & Reflection' requirement: The agent thinks through its approach, then reflects on its output. ğŸ“‹ Reasoning Steps (Agent's Approach): 1. ```json { "reasoning_steps": [ "Step 1: Review the repository structure to identify where langgraph is utilized. This includes examining the modules and files to locate any references to langgraph.", "Step 2: Analyze the dependencies to see if langgraph is explicitly listed as a dependency and understand its role within the project. This may involve checking documentation or comments within the code.", "Step 3: Investigate the identified 11 modules to determine which ones make use of langgraph and the specific functionalities it provides within those modules.", "Step 4: Assess the purpose of langgraph in the context of the overall project by evaluating how it interacts with other components and contributes to the project goals.", "Step 5: Compile findings into a summary that explains where langgraph is used, why it is important, and how it enhances the functionality of the repository." ] } ``` âœ… Agent Decision: Output quality is good, proceeding to evaluation ====================================================================== ğŸ“Š EVALUATION SCORES ====================================================================== Overall Score: 74.5/100 â€¢ Task Completion: 90.0 â€¢ Reasoning Quality: 100.0 â€¢ Tool Effectiveness: 20.0 â€¢ Reflection Quality: 45.0 â€¢ Output Quality: 70.0 ---------------------------------------------------------------------- ğŸ“‹ WHY THESE SCORES? ---------------------------------------------------------------------- ====================================================================== ğŸ‘¤ You: Why did we choose langgraph vs langchain ? ğŸ¤– Agent: Thinking... ğŸ“š Retrieving from knowledge base (ChromaDB)... Vector database collection 'documents' ready at ./chroma_db âœ“ Knowledge base has 43 documents âœ“ Retrieved 3 relevant chunks ğŸ§  Performing LLM-based reasoning... âœ“ Generated 5 reasoning steps ğŸ¤– Generating response (attempt 1)... âœ“ Generated 690 characters ğŸ” Performing self-reflection on generated output... âœ“ Self-assessment: good ğŸ’­ Reasoning: Output answers the question with available data âœ… Action: Output quality is good, proceeding to evaluation ====================================================================== âœ… AGENT RESPONSE ====================================================================== We chose Langgraph over Langchain primarily because Langgraph offers better integration with graph systems and enhances the reliability of our solutions when combined with language models (LM). Langgraph allows us to effectively manage connections and relationships within data, which is crucial for handling complex queries, especially in scenarios requiring multi-hop reasoning. Additionally, the ease of installation and usability of Langgraph, particularly with graph databases like Neo4j, made it a more practical choice for our needs. Overall, Langgraph's capabilities in handling graph representations and connections align better with our project objectives compared to Langchain. ====================================================================== ğŸ§  AGENT REASONING & SELF-REFLECTION PROCESS ====================================================================== This demonstrates the Capstone 'Reasoning & Reflection' requirement: The agent thinks through its approach, then reflects on its output. ğŸ“‹ Reasoning Steps (Agent's Approach): 1. Step 1: Identify the core functionalities and features of langgraph and langchain to understand what each tool offers. 2. Step 2: Assess the specific needs and requirements of the project to determine which tool aligns better with those needs. 3. Step 3: Consider factors such as community support, documentation quality, ease of use, and integration capabilities of both langgraph and langchain. 4. Step 4: Evaluate any performance metrics, user reviews, or case studies that highlight the effectiveness of langgraph versus langchain in real-world applications. 5. Step 5: Make a comparative analysis to justify the decision based on the gathered information and the project's objectives. âœ… Agent Decision: Output quality is good, proceeding to evaluation ====================================================================== ğŸ“Š EVALUATION SCORES ====================================================================== Overall Score: 86.5/100 â€¢ Task Completion: 90.0 â€¢ Reasoning Quality: 100.0 â€¢ Tool Effectiveness: 80.0 â€¢ Reflection Quality: 45.0 â€¢ Output Quality: 90.0 ---------------------------------------------------------------------- ğŸ“‹ WHY THESE SCORES? ---------------------------------------------------------------------- âœ¨ Output Quality (90.0/ 100): ğŸ“‹ Task Type: GENERAL (basic evaluation) âœ… Has output (690 chars, 40/40 pts) âœ… Appropriate length (30/30 pts) âœ… Has structure (20/20 pts) ====================================================================== ğŸ‘¤ You: Write me linkedin post about this repo and mention that Mr Tomato inspired me ğŸ’¡ LinkedIn post about repository - will use cached data ğŸ¤– Agent: Thinking... ğŸ“¦ Using cached repository data from previous query ğŸ§  Performing LLM-based reasoning... âœ“ Generated 3 reasoning steps ğŸ¤– Generating response (attempt 1)... âœ“ Generated 1327 characters ğŸ” Performing self-reflection on generated output... âœ“ Self-assessment: good ğŸ’­ Reasoning: The post effectively highlights the project and acknowledges Mr. Tomato's inspiration, maintaining a professional tone suitable for LinkedIn. âœ… Action: Output quality is good, proceeding to evaluation ====================================================================== âœ… AGENT RESPONSE ====================================================================== ğŸš€ Exciting news from the Ciklum AI Academy! Introducing Simple-RAG v2.0, our cutting-edge autonomous AI agent system that marks a major evolution from a basic RAG chatbot to an intelligent, agentic AI system! ğŸ¤–âœ¨ This innovative project boasts a powerful LangGraph orchestration, featuring 6 intelligent nodes, and itâ€™s equipped with autonomous repository analysis capabilities. With multi-step reasoning powered by chain-of-thought and a robust self-reflection mechanism for quality assurance, Simple-RAG v2.0 truly sets a new standard. Our 5-metric evaluation framework ensures task completion, reasoning quality, tool effectiveness, reflection quality, and output quality are all up to par, backed by a Test-Driven Development approach that guarantees a 100% test pass rate! ğŸ¯ Built on a solid technical stack utilizing LangGraph, LangChain, Azure OpenAI, ChromaDB, and GPT-4o, this project emphasizes autonomous behavior and self-evaluation, resulting in production-ready code thatâ€™s ready for real-world applications. A special shoutout to Mr. Tomato for the inspiration that fueled this journey! ğŸ… Iâ€™m incredibly grateful to the @Ciklum and AI Academy team for their support and collaboration. Curious to see more? Check out the GitHub repo! #CiklumAIAcademy #AIEngineering #AutonomousAI #LangGraph #MachineLearning ====================================================================== ğŸ§  AGENT REASONING & SELF-REFLECTION PROCESS ====================================================================== This demonstrates the Capstone 'Reasoning & Reflection' requirement: The agent thinks through its approach, then reflects on its output. ğŸ“‹ Reasoning Steps (Agent's Approach): 1. Step 1: Begin by outlining the purpose of the LinkedIn post, which is to highlight the achievements of the repository and acknowledge Mr. Tomato for the inspiration. 2. Step 2: Gather key information about the repository, including the number of items (73), dependencies (25), and modules (11) to showcase its complexity and functionality. 3. Step 3: Draft the post with a clear introduction that mentions Mr. Tomato, followed by a summary of the repository's features and its significance. âœ… Agent Decision: Output quality is good, proceeding to evaluation ====================================================================== ğŸ“Š EVALUATION SCORES ====================================================================== Overall Score: 79.0/100 â€¢ Task Completion: 90.0 â€¢ Reasoning Quality: 100.0 â€¢ Tool Effectiveness: 20.0 â€¢ Reflection Quality: 45.0 â€¢ Output Quality: 100.0 ---------------------------------------------------------------------- ğŸ“‹ WHY THESE SCORES? ---------------------------------------------------------------------- âœ¨ Output Quality (100.0/ 100): ğŸ“‹ Task Type: LINKEDIN POST (specialized evaluation) Professional Structure (30 pts): âœ… Engaging opening (10/10 pts) âœ… Technical features section (10/10 pts) âœ… Call-to-action closing (10/10 pts) Content Quality (40 pts): âœ… Excellent hashtag usage (5 hashtags, 10/10 pts) âœ… Engaging emojis (4 types, 10/10 pts) âœ… Technical specificity (20/20 pts) Accuracy & Authenticity (30 pts): âœ… Repo stats included (15/15 pts) âœ… Self-reflection demo (15/15 pts) ======================================================================



Be a real Critic of this REPO, we need to excel on given task (pasted below). The output for the tests (also below). Please Deep dive into the code, analyze it, make sure it fulfill all of the requirements of the tasks and excels them, there are no bugs, it is properly adjusted for best performance and token savings (at least decent in both of this). Feel free to test it on your own as well and ensure it indeed works as it should (but im pasing output below so reuse it to be most efficient)

TASK:

ğŸ“š Engineers Capstone Project [Ciklum AI Academy] {#ğŸ“š-engineers-capstone-project-[ciklum-ai-academy]}
Select the section youâ€™d like to explore from the list to jump right to it.

ğŸ“š Engineers Capstone Project [Ciklum AI Academy] 1

ğŸ“˜ Project Overview 2

ğŸ¯ Objective 2

ğŸ“˜ What to Do 2

ğŸ§° Technology Stack (flexible): 3

ğŸ“¦ Deliverables 3

ğŸ“ Evaluation Criteria 4

Dear Learner,

First of all, congratulations on reaching this point! Youâ€™re almost there! Youâ€™ve shown dedication, curiosity, and a willingness to grow in one of the most transformative areas of technology.

At Ciklum, we believe that continuous learning is the real driver of progress, and that those who keep evolving will lead the future of tech adoption.

Thank you for being part of this journey and for the effort youâ€™ve invested along the way.

We focus on providing you with the most essential skills and experience needed for real-world practical AI development.

The Ciklum AI Academy is committed to supporting your growth and helping you turn knowledge into impact. When each of us grows, we all grow together.

With appreciation,
The AI Academy Team

ğŸ“˜ Project Overview {#ğŸ“˜-project-overview}
This is your final assignment in the AI Academy Engineering Track.

In this version, weâ€™ve placed greater emphasis on the real-world foundations of AI-Agentic system development.

Our goal is to ensure you cover 80â€“90% of the core components involved in building modern AI agent systems. Your task is to design and implement an AI-Agentic system that addresses a real problem faced by you, your team, or the broader Ciklum community. Some example options are available here.

Youâ€™ll work through every major stage, including:

Data preparation and contextualization
RAG pipeline design for information retrieval
AI self-reflection and reasoning
Tool-calling mechanisms, enabling the agent to take meaningful actions and self-reflect on its performance
Evaluation and measurement of your agentâ€™s effectiveness
A demo video showcasing your system in action
This assignment combines engineering depth, system design, and practical evaluation, reflecting how modern agentic AI systems are built, tested, and deployed in real-world environments.

ğŸ¯ Objective {#ğŸ¯-objective}
Build upon your previous RAG chatbot and data preparation work (HW4) to develop an AI-Agentic system capable of performing autonomous reasoning, taking meaningful tool-based actions, and reflecting on its own decisions and performance.

The goal is to demonstrate your ability to design a technically robust, self-reflective agentic system that integrates data retrieval, reasoning, action execution, and evaluation into a cohesive workflow.

ğŸ“˜ What to Do {#ğŸ“˜-what-to-do}
Work through all major components of an agentic AI system:

Data Preparation & Contextualization â€“ prepare relevant data your agent will use.
RAG Pipeline Design â€“ design your retrieval mechanism (e.g., embeddings + vector store).
Reasoning & Reflection â€“ ensure your agent can analyze and self-correct.
Tool-Calling Mechanisms â€“ allow it to take actions or run tools based on reasoning.
Evaluation â€“ include a simple way to measure success (accuracy, relevance, clarity).
ğŸ§° Technology Stack (flexible): {#ğŸ§°-technology-stack-(flexible):}
Use any preferred stack â€” Python, LangChain, CrewAI, OpenAI API, or a custom setup.
Keep it simple but functional: focus on reasoning, inspection, and message generation rather than complexity.

Your agent may use any preferred stack or framework (e.g., Python + OpenAI API, LangChain, CrewAI, or custom logic). It should be simple yet functional, focusing on reasoning, file inspection, and message generation rather than complexity.

ğŸ“¦ Deliverables {#ğŸ“¦-deliverables}
To complete the assignment, submit the following:

Git Repository Link â€“ A publicly accessible GitHub repository containing your working agentâ€™s source code.

Include a short README.md and an architecture.mmd file that describes how to run the agent and what technologies or libraries you used.
The repository should contain enough documentation for mentors to test or review the logic.
A Demo video containing the overview and demonstration of your solution at work

Generated Post (Optional) â€“ A short social-style message created by your agent itself, published on LinkedIn or another platform of your choice.

The post should explain, in the agentâ€™s own words:

What it does and how it was built
That it was created as part of the Ciklum AI Academy
(Optional) Include a mention or tag for @Ciklum
Keep it professional, authentic, and concise (5â€“7 sentences).

Submission Form â€“ Complete the official AI Academy submission form and include:

Link to your Git repository
Link to a short ~5 min demo video recording.
Link to your public post
Any additional comments or notes youâ€™d like to share with the review team
Make sure all links are public and accessible.
ğŸ“ Evaluation Criteria {#ğŸ“-evaluation-criteria}
Criterion	Description
Functionality	The agent runs and successfully analyses its repository.
Creativity & Implementation	Thoughtful design choices, clear logic, and intelligent message generation.
Relevance	The agent clearly references its context (AI Academy, Ciklum).
Presentation	The generated post is coherent, professional, and aligned with the brief.
Accessibility	Links are valid, publicly viewable, and well-documented.
Well done!ğŸ‘

Output:
python interactive_agent.py

ğŸš€ Starting Simple-RAG v2.0 Interactive Agent...

======================================================================
ğŸ¤– Simple-RAG v2.0 - Interactive Agent CLI
Autonomous AI Agent with Self-Reflection
ğŸ’¡ Tips:

Ask the agent to analyze this repository
Request explanations of the codebase
Generate content or LinkedIn posts
Ask about specific components
ğŸ“ Commands:

Type 'exit' or 'quit' to end session
Type 'clear' to clear screen
Type 'history' to see conversation history
Type 'stats' to see session statistics
======================================================================
ğŸ‘¤ You: What is this repo about?

ğŸ¤– Agent: Thinking...

ğŸ” Analyzing repository at: /Users/bb/Programming/Simple-RAGv1/Simple-RAG
âœ“ Found 73 top-level items
âœ“ Analyzed 20 source files
âœ“ Identified 25 dependencies
âœ“ Mapped 11 modules
âœ“ Extracted 92 code symbols (21 classes, 71 functions)
Note: 82 test functions found via AST (pytest will find more including parametrized tests)
âœ“ Ran pytest --collect-only
ğŸ“¦ Using cached coverage data...
âœ“ Coverage: 32% (cached)
âœ“ Counted 24 test files
ğŸ§  Performing LLM-based reasoning...
âœ“ Generated 5 reasoning steps
ğŸ¤– Generating response (attempt 1)...
âœ“ Generated 2209 characters
ğŸ” Performing self-reflection on generated output...
âœ“ Self-assessment: good
ğŸ’­ Reasoning:
âœ… Action: Output quality is good, proceeding to evaluation

======================================================================
âœ… AGENT RESPONSE
Summary
The repository implements an AI agent framework for processing and interpreting documents [evidence: AGENT_FLOW_DOCUMENTATION.md].
It includes an interactive command-line interface through the AgentCLI class [evidence: interactive_agent.py:12].
The framework utilizes various AI technologies, including Azure's Form Recognizer for document analysis [evidence: requirements.txt].
The project contains 215 tests collected to ensure functionality [evidence: pytest --collect-only output].
It features multiple modules for handling different aspects of agent functionality, such as evaluation, agent nodes, and configuration [evidence: code_symbols].
Repository Structure
73 items:
- .coverage
- .dockerignore
- .env
- .env.example
- AGENT_FLOW_DOCUMENTATION.md
- BEFORE_AFTER_REFLECTION_DEMO.md
- BUG_FIX_CODE_QUESTIONS.md
- BUG_FIX_EXCESSIVE_REASONING_STEPS.md
- CEO_EVIDENCE_ONLY_VERIFICATION.md
- CIKLUM_DEMO_SEQUENCES.md
- requirements.txt
- src/
- tests/
Key Modules & Entry Points
src/agent/orchestrator.py [evidence: code_symbols]

AgentRunner class at line 45 [evidence: src/agent/orchestrator.py:45]
run_agent() function at line 120 [evidence: src/agent/orchestrator.py:120]
Entry point: if __name__ == "__main__": at line 180 [evidence: src/agent/orchestrator.py:180]
interactive_agent.py [evidence: code_symbols]

AgentCLI class at line 12 [evidence: interactive_agent.py:12]
Tests & Quality Signals
Test Count: 215 tests collected [evidence: pytest --collect-only output]
Test Files: 24 test files found [evidence: find tests command]
Coverage: 32% coverage [evidence: coverage report]
Linters: No .pylintrc or pyproject.toml [tool] section found [evidence: file search]
Dependencies (from requirements.txt)
pytest==6.2.5 - Testing framework [evidence: requirements.txt]
azure-ai-formrecognizer - Document analysis [evidence: requirements.txt]
System Capabilities (Evidence-Based)
Document Processing: Proven by DocumentProcessor class in src/processor.py at line 30 [evidence: code_symbols].
Gaps / Unknowns
Type hints presence unknown - requires manual inspection.
======================================================================
ğŸ§  AGENT REASONING & SELF-REFLECTION PROCESS
This demonstrates the Capstone 'Reasoning & Reflection' requirement:
The agent thinks through its approach, then reflects on its output.

ğŸ“‹ Reasoning Steps (Agent's Approach):

Step 1: Review the repository's README file and documentation to understand its purpose, usage, and installation instructions. This often provides a high-level overview of what the repo is about.
Step 2: Analyze the identified modules to determine their functionalities and how they interact with each other, which can give insight into the overall structure and purpose of the repository.
Step 3: Look at the dependencies to understand the technologies and frameworks used, which can provide context about the type of application or library the repository is implementing.
Step 4: Check the commit history and issues to see the development activity and any user feedback, which can indicate the maturity and focus areas of the project.
Step 5: If necessary, explore example files or tests within the repository to see practical applications or use cases, further clarifying what the repo accomplishes.
âœ… Agent Decision: Output quality is good, proceeding to evaluation

======================================================================
ğŸ“Š EVALUATION SCORES
Overall Score: 88.6/100
â€¢ Task Completion: 90.0
â€¢ Reasoning Quality: 100.0
â€¢ Tool Effectiveness: 100.0
â€¢ Reflection Quality: 45.0
â€¢ Output Quality: 84.0

ğŸ“‹ WHY THESE SCORES?
âœ¨ Tool Effectiveness (100.0/100):
âœ… Used 16 tool calls (50/50 pts)
âœ… Multiple diverse tool calls - 3+ (30/30 pts)
âœ… Tools produced comprehensive results (20/20 pts)
ğŸ‘¤ You: Where do we use langgraph and why?

ğŸ¤– Agent: Thinking...

ğŸ“¦ Using cached repository data from previous query
ğŸ“¦ Using fully cached repository analysis (skipping expensive operations)
âœ“ Repo structure: 73 items (cached)
âœ“ Code files: 20 files (cached)
âœ“ Dependencies: 25 (cached)
âœ“ Architecture: 11 modules (cached)
âœ“ Code symbols: 21 classes, 71 functions (cached)
âœ“ Pytest collection (cached)
âœ“ Coverage report (cached)
ğŸ§  Performing LLM-based reasoning...
âœ“ Generated 1 reasoning steps
ğŸ“„ Detected code-specific question - including source code excerpts
ğŸ“‹ Found 11 relevant files for query
Including: orchestrator.py
Including: evaluator.py
Including: planner.py
ğŸ¤– Generating response (attempt 1)...
âœ“ Generated 697 characters
ğŸ” Performing self-reflection on generated output...
âœ“ Self-assessment: good
ğŸ’­ Reasoning:
âœ… Action: Output quality is good, proceeding to evaluation

======================================================================
âœ… AGENT RESPONSE
langgraph is used in the following file:

File: /Users/bb/Programming/Simple-RAGv1/Simple-RAG/src/agent/orchestrator.py
Line 8: from langgraph.graph import StateGraph, END
It is utilized in the function create_agent_graph() (defined at line 19) to create a state graph that likely manages the flow and state of the agent within the system.
Relevant Code Excerpt:
# In orchestrator.py
from langgraph.graph import StateGraph, END  # Line 8

def create_agent_graph() -> StateGraph:  # Line 19
langgraph is integral for structuring and managing the state transitions within the agent's operational framework, allowing for complex decision-making processes.

======================================================================
ğŸ§  AGENT REASONING & SELF-REFLECTION PROCESS
This demonstrates the Capstone 'Reasoning & Reflection' requirement:
The agent thinks through its approach, then reflects on its output.

ğŸ“‹ Reasoning Steps (Agent's Approach):

{
"reasoning_steps": [
"Step 1: Review the repository structure to identify where langgraph is utilized. This includes examining the modules and files to locate any references to langgraph.",
"Step 2: Analyze the dependencies to see if langgraph is explicitly listed as a dependency and understand its role within the project. This may involve checking documentation or comments within the code.",
"Step 3: Investigate the identified 11 modules to determine which ones make use of langgraph and the specific functionalities it provides within those modules.",
"Step 4: Assess the purpose of langgraph in the context of the overall project by evaluating how it interacts with other components and contributes to the project goals.",
"Step 5: Compile findings into a summary that explains where langgraph is used, why it is important, and how it enhances the functionality of the repository."
]
}


âœ… Agent Decision: Output quality is good, proceeding to evaluation

======================================================================
ğŸ“Š EVALUATION SCORES
======================================================================
Overall Score: 74.5/100
  â€¢ Task Completion: 90.0
  â€¢ Reasoning Quality: 100.0
  â€¢ Tool Effectiveness: 20.0
  â€¢ Reflection Quality: 45.0
  â€¢ Output Quality: 70.0

----------------------------------------------------------------------
ğŸ“‹ WHY THESE SCORES?
----------------------------------------------------------------------
======================================================================

ğŸ‘¤ You: Why did we choose langgraph vs langchain ?

ğŸ¤– Agent: Thinking...

ğŸ“š Retrieving from knowledge base (ChromaDB)...
Vector database collection 'documents' ready at ./chroma_db
  âœ“ Knowledge base has 43 documents
  âœ“ Retrieved 3 relevant chunks
ğŸ§  Performing LLM-based reasoning...
  âœ“ Generated 5 reasoning steps
  ğŸ¤– Generating response (attempt 1)...
  âœ“ Generated 690 characters
ğŸ” Performing self-reflection on generated output...
  âœ“ Self-assessment: good
  ğŸ’­ Reasoning: Output answers the question with available data
  âœ… Action: Output quality is good, proceeding to evaluation

======================================================================
âœ… AGENT RESPONSE
======================================================================
We chose Langgraph over Langchain primarily because Langgraph offers better integration with graph systems and enhances the reliability of our solutions when combined with language models (LM). Langgraph allows us to effectively manage connections and relationships within data, which is crucial for handling complex queries, especially in scenarios requiring multi-hop reasoning. 

Additionally, the ease of installation and usability of Langgraph, particularly with graph databases like Neo4j, made it a more practical choice for our needs. Overall, Langgraph's capabilities in handling graph representations and connections align better with our project objectives compared to Langchain.

======================================================================
ğŸ§  AGENT REASONING & SELF-REFLECTION PROCESS
======================================================================

This demonstrates the Capstone 'Reasoning & Reflection' requirement:
The agent thinks through its approach, then reflects on its output.

ğŸ“‹ Reasoning Steps (Agent's Approach):
  1. Step 1: Identify the core functionalities and features of langgraph and langchain to understand what each tool offers.
  2. Step 2: Assess the specific needs and requirements of the project to determine which tool aligns better with those needs.
  3. Step 3: Consider factors such as community support, documentation quality, ease of use, and integration capabilities of both langgraph and langchain.
  4. Step 4: Evaluate any performance metrics, user reviews, or case studies that highlight the effectiveness of langgraph versus langchain in real-world applications.
  5. Step 5: Make a comparative analysis to justify the decision based on the gathered information and the project's objectives.

âœ… Agent Decision: Output quality is good, proceeding to evaluation

======================================================================
ğŸ“Š EVALUATION SCORES
======================================================================
Overall Score: 86.5/100
  â€¢ Task Completion: 90.0
  â€¢ Reasoning Quality: 100.0
  â€¢ Tool Effectiveness: 80.0
  â€¢ Reflection Quality: 45.0
  â€¢ Output Quality: 90.0

----------------------------------------------------------------------
ğŸ“‹ WHY THESE SCORES?
----------------------------------------------------------------------

âœ¨ Output Quality (90.0/ 100):
  ğŸ“‹ Task Type: GENERAL (basic evaluation)
     âœ… Has output (690 chars, 40/40 pts)
     âœ… Appropriate length (30/30 pts)
     âœ… Has structure (20/20 pts)
======================================================================

ğŸ‘¤ You: Write me linkedin post about this repo and mention that Mr Tomato inspired me
  ğŸ’¡ LinkedIn post about repository - will use cached data

ğŸ¤– Agent: Thinking...

  ğŸ“¦ Using cached repository data from previous query
ğŸ§  Performing LLM-based reasoning...
  âœ“ Generated 3 reasoning steps
  ğŸ¤– Generating response (attempt 1)...
  âœ“ Generated 1327 characters
ğŸ” Performing self-reflection on generated output...
  âœ“ Self-assessment: good
  ğŸ’­ Reasoning: The post effectively highlights the project and acknowledges Mr. Tomato's inspiration, maintaining a professional tone suitable for LinkedIn.
  âœ… Action: Output quality is good, proceeding to evaluation

======================================================================
âœ… AGENT RESPONSE
======================================================================
ğŸš€ Exciting news from the Ciklum AI Academy! Introducing Simple-RAG v2.0, our cutting-edge autonomous AI agent system that marks a major evolution from a basic RAG chatbot to an intelligent, agentic AI system! ğŸ¤–âœ¨

This innovative project boasts a powerful LangGraph orchestration, featuring 6 intelligent nodes, and itâ€™s equipped with autonomous repository analysis capabilities. With multi-step reasoning powered by chain-of-thought and a robust self-reflection mechanism for quality assurance, Simple-RAG v2.0 truly sets a new standard. Our 5-metric evaluation framework ensures task completion, reasoning quality, tool effectiveness, reflection quality, and output quality are all up to par, backed by a Test-Driven Development approach that guarantees a 100% test pass rate! ğŸ¯

Built on a solid technical stack utilizing LangGraph, LangChain, Azure OpenAI, ChromaDB, and GPT-4o, this project emphasizes autonomous behavior and self-evaluation, resulting in production-ready code thatâ€™s ready for real-world applications.

A special shoutout to Mr. Tomato for the inspiration that fueled this journey! ğŸ… Iâ€™m incredibly grateful to the @Ciklum and AI Academy team for their support and collaboration. 

Curious to see more? Check out the GitHub repo! 

#CiklumAIAcademy #AIEngineering #AutonomousAI #LangGraph #MachineLearning

======================================================================
ğŸ§  AGENT REASONING & SELF-REFLECTION PROCESS
======================================================================

This demonstrates the Capstone 'Reasoning & Reflection' requirement:
The agent thinks through its approach, then reflects on its output.

ğŸ“‹ Reasoning Steps (Agent's Approach):
  1. Step 1: Begin by outlining the purpose of the LinkedIn post, which is to highlight the achievements of the repository and acknowledge Mr. Tomato for the inspiration.
  2. Step 2: Gather key information about the repository, including the number of items (73), dependencies (25), and modules (11) to showcase its complexity and functionality.
  3. Step 3: Draft the post with a clear introduction that mentions Mr. Tomato, followed by a summary of the repository's features and its significance.

âœ… Agent Decision: Output quality is good, proceeding to evaluation

======================================================================
ğŸ“Š EVALUATION SCORES
======================================================================
Overall Score: 79.0/100
  â€¢ Task Completion: 90.0
  â€¢ Reasoning Quality: 100.0
  â€¢ Tool Effectiveness: 20.0
  â€¢ Reflection Quality: 45.0
  â€¢ Output Quality: 100.0

----------------------------------------------------------------------
ğŸ“‹ WHY THESE SCORES?
----------------------------------------------------------------------

âœ¨ Output Quality (100.0/ 100):
  ğŸ“‹ Task Type: LINKEDIN POST (specialized evaluation)
  
  Professional Structure (30 pts):
     âœ… Engaging opening (10/10 pts)
     âœ… Technical features section (10/10 pts)
     âœ… Call-to-action closing (10/10 pts)
  
  Content Quality (40 pts):
     âœ… Excellent hashtag usage (5 hashtags, 10/10 pts)
     âœ… Engaging emojis (4 types, 10/10 pts)
     âœ… Technical specificity (20/20 pts)
  
  Accuracy & Authenticity (30 pts):
     âœ… Repo stats included (15/15 pts)
     âœ… Self-reflection demo (15/15 pts)
======================================================================




Try again






Auto context




[ğŸ“˜ Project Overview	2](#ğŸ“˜-project-overview)

[ğŸ¯ Objective	2](#ğŸ¯-objective)

[ğŸ“˜ What to Do	2](#ğŸ“˜-what-to-do)

[ğŸ§° Technology Stack (flexible):	3](#ğŸ§°-technology-stack-\(flexible\):)

[ğŸ“¦ Deliverables	3](#ğŸ“¦-deliverables)

[ğŸ“ Evaluation Criteria	4](#ğŸ“-evaluation-criteria)

## 

Dear Learner,

First of all, congratulations on reaching this point\! Youâ€™re almost there\! Youâ€™ve shown dedication, curiosity, and a willingness to grow in one of the most transformative areas of technology.

At Ciklum, we believe that **continuous learning is the real driver of progress**, and that **those who keep evolving will lead the future of tech adoption**. 

Thank you for being part of this journey and for the effort youâ€™ve invested along the way.

We focus on providing you with the most essential skills and experience needed for real-world practical AI development.

The **Ciklum AI Academy** is committed to supporting your growth and helping you turn knowledge into impact. When each of us grows, we all grow together.

With appreciation,  
[**The AI Academy Team**](https://sites.google.com/ciklum.com/ai-academy-2025/meet-the-team)

## 

### ğŸ“˜ Project Overview {#ğŸ“˜-project-overview}

This is your final assignment in the AI Academy Engineering Track.

In this version, weâ€™ve placed greater emphasis on the real-world foundations of AI-Agentic system development.

Our goal is to ensure you cover **80â€“90% of the core components** involved in building modern AI agent systems. Your task is to **design and implement an AI-Agentic system** that addresses a real problem faced by you, your team, or the broader **Ciklum community**. Some example options are available [here](https://docs.google.com/spreadsheets/d/1AKtuGHM1hVwB9kkDM8vUYqw6I_LiN_SJG2RRWg8DzXk/edit?gid=0#gid=0).

Youâ€™ll work through every major stage, including:

* Data preparation and contextualization  
* RAG pipeline design for information retrieval  
* AI self-reflection and reasoning  
* Tool-calling mechanisms, enabling the agent to take meaningful actions and self-reflect on its performance  
* Evaluation and measurement of your agentâ€™s effectiveness  
* A demo video showcasing your system in action

This assignment combines engineering depth, system design, and practical evaluation, reflecting how modern agentic AI systems are built, tested, and deployed in real-world environments.

### 

### ğŸ¯ Objective {#ğŸ¯-objective}

Build upon your previous **RAG chatbot and data preparation work** *(HW4)* to develop an **AI-Agentic system** capable of performing autonomous reasoning, taking meaningful tool-based actions, and reflecting on its own decisions and performance. 

The goal is to demonstrate your ability to design a **technically robust, self-reflective agentic system** that integrates data retrieval, reasoning, action execution, and evaluation into a cohesive workflow.

#### **ğŸ“˜ What to Do** {#ğŸ“˜-what-to-do}

Work through all major components of an agentic AI system:

1. Data Preparation & Contextualization â€“ prepare relevant data your agent will use.  
2. RAG Pipeline Design â€“ design your retrieval mechanism (e.g., embeddings \+ vector store).  
3. Reasoning & Reflection â€“ ensure your agent can analyze and self-correct.  
4. Tool-Calling Mechanisms â€“ allow it to take actions or run tools based on reasoning.  
5. Evaluation â€“ include a simple way to measure success (accuracy, relevance, clarity).

#### ğŸ§° Technology Stack (flexible): {#ğŸ§°-technology-stack-(flexible):}

Use any preferred stack â€” Python, LangChain, CrewAI, OpenAI API, or a custom setup.  
Keep it simple but functional: focus on reasoning, inspection, and message generation rather than complexity.

Your agent may use any **preferred stack or framework** (e.g., Python \+ OpenAI API, LangChain, CrewAI, or custom logic). It should be simple yet functional, focusing on reasoning, file inspection, and message generation rather than complexity.

#### ğŸ“¦ Deliverables {#ğŸ“¦-deliverables}

To complete the assignment, submit the following:

1. **Git Repository Link** â€“ A publicly accessible GitHub repository containing your working agentâ€™s source code.

   * Include a short **README.md** and an **architecture.mmd** file that describes how to run the agent and what technologies or libraries you used.  
   * The repository should contain enough documentation for mentors to test or review the logic.

2. A **Demo video** containing the overview and demonstration of your solution at work  
3. **Generated Post** (Optional) â€“ A short social-style message created **by your agent itself**, published on **LinkedIn** or another platform of your choice.

   * The post should explain, in the agentâ€™s own words:  
     * What it does and how it was built  
     * That it was created as part of the *Ciklum AI Academy*  
     * (Optional) Include a mention or tag for **@Ciklum**

   * Keep it professional, authentic, and concise (5â€“7 sentences).

4. **Submission Form** â€“ Complete the official [**AI Academy submission form**](https://forms.gle/4ZftZA2x3XNaHjzs5) and include:

   * Link to your Git repository  
   * Link to a short \~5 min demo video recording.  
   * Link to your public post  
   * Any additional comments or notes youâ€™d like to share with the review team  
   * **Make sure all links are public and accessible.**  
     

### ğŸ“ Evaluation Criteria {#ğŸ“-evaluation-criteria}

| Criterion | Description |
| ----- | ----- |
| **Functionality** | The agent runs and successfully analyses its repository. |
| **Creativity & Implementation** | Thoughtful design choices, clear logic, and intelligent message generation. |
| **Relevance** | The agent clearly references its context (AI Academy, Ciklum). |
| **Presentation** | The generated post is coherent, professional, and aligned with the brief. |
| **Accessibility** | Links are valid, publicly viewable, and well-documented. |

Well done\!ğŸ‘