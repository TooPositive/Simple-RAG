# Simple-RAG v2.0: Autonomous AI Agent System

**Version**: 2.0.0  
**Status**: Production Ready âœ…  
**Test Coverage**: 107/107 tests passing (100%)

An autonomous AI agent built on top of the Simple-RAG v1.0 foundation, featuring LangGraph orchestration, self-reflection, repository analysis, and comprehensive evaluation.

---

## ğŸŒŸ What's New in v2.0

### Autonomous Agent Capabilities
- **LangGraph Orchestration**: State-based workflow with conditional routing
- **Self-Reflection with Proof**: Agent critiques reasoning and shows BEFORE/AFTER improvements
- **Repository Analysis**: Autonomous codebase understanding with 4 specialized tools
- **Multi-Step Reasoning**: Chain-of-thought processing with visible steps
- **Comprehensive Evaluation**: 5-metric scoring system with transparent explanations
- **RAG Integration**: ChromaDB retrieval node for knowledge base questions

### Architecture Enhancements
- **Agent State Management**: TypedDict-based state flow with 15+ tracked fields
- **Intelligent Nodes**: 6 specialized nodes (Planning, Analysis, Retrieval, Reasoning, Reflection, Generation)
- **Tool Integration**: Repository analysis + RAG retrieval + generation tools
- **Evaluation Framework**: Task-aware quantitative assessment with detailed explanations
- **Context Management**: Smart caching and follow-up question handling

### Production-Quality Features âœ¨
- **Transparent Evaluation Explanations**: Shows WHY each score was earned
- **BEFORE/AFTER Self-Reflection Demo**: Proves self-correction with side-by-side comparison
- **Critical Self-Assessment**: Authentic critique that only shows improvements when they exist
- **Evidence-Based Analysis**: Repository analysis demands concrete code references, test citations, and versions
- **Rigorous Evaluation**: Penalizes vague language, rewards specific evidence
- **Interactive CLI**: Context-aware conversations with visible reasoning

---

## ğŸ—ï¸ v2.0 Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Task Input                       â”‚
â”‚         (Interactive CLI with Context Tracking)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Planning Node                           â”‚
â”‚  â€¢ Analyzes task type (code/RAG/content/general)        â”‚
â”‚  â€¢ Determines execution strategy                         â”‚
â”‚  â€¢ Routes to appropriate action                          â”‚
â”‚  â€¢ Uses cached data when available                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                        â†“           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ Repository       â”‚    â”‚ RAG Retrieval    â”‚  â”‚ Direct
â”‚ Analyzer Node    â”‚    â”‚ Node (NEW!)      â”‚  â”‚ Reasoning
â”‚ â€¢ Structure      â”‚    â”‚ â€¢ ChromaDB query â”‚  â”‚
â”‚ â€¢ File content   â”‚    â”‚ â€¢ Vector search  â”‚  â”‚
â”‚ â€¢ Dependencies   â”‚    â”‚ â€¢ Top 3 chunks   â”‚  â”‚
â”‚ â€¢ Architecture   â”‚    â”‚ â€¢ Knowledge base â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Reasoning Node                          â”‚
â”‚  â€¢ Chain-of-thought processing (3-5 steps)              â”‚
â”‚  â€¢ Multi-step analysis with GPT-4o (temp=0.7)           â”‚
â”‚  â€¢ Information synthesis from all sources                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Reflection Node                         â”‚
â”‚  â€¢ CRITICAL self-critique (temp=0.3)                    â”‚
â”‚  â€¢ Evidence checks: Demands code symbols, test refs     â”‚
â”‚  â€¢ Quality assessment: "good" or "needs_improvement"     â”‚
â”‚  â€¢ Finds real gaps and weaknesses                        â”‚
â”‚  â€¢ Always proceeds to generation                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Generation Node                         â”‚
â”‚  IF needs_improvement:                                   â”‚
â”‚    â€¢ Generate BEFORE (without reflection) - baseline    â”‚
â”‚    â€¢ Generate AFTER (with reflection) - improved        â”‚
â”‚    â€¢ Show side-by-side comparison                        â”‚
â”‚  ELSE (reflection said "good"):                          â”‚
â”‚    â€¢ Generate once (no fake improvements)                â”‚
â”‚  â€¢ Evidence-based prompts: Demands concrete code refs   â”‚
â”‚  â€¢ Prohibits vague language (likely, suggests, etc.)    â”‚
â”‚  â€¢ Synthesizes using all context                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Evaluation Node (NEW!)                  â”‚
â”‚  â€¢ Task-aware scoring (code vs LinkedIn vs repo)        â”‚
â”‚  â€¢ 5 metrics with detailed explanations:                â”‚
â”‚    - Task completion (35%)                               â”‚
â”‚    - Reasoning quality (25%)                             â”‚
â”‚    - Tool effectiveness (15%)                            â”‚
â”‚    - Reflection quality (10%)                            â”‚
â”‚    - Output quality (15%)                                â”‚
â”‚  â€¢ "WHY THESE SCORES?" breakdown shown                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
       Final Output + Scores + Explanations
              (Transparent & Verifiable)
```

---

## ğŸ¯ Evidence-Based Repository Analysis

**NEW in v2.0**: Rigorous, professional-grade code analysis that demands concrete evidence instead of vague assumptions.

### ğŸ” Understanding Test Counts

**Why you see two different test numbers:**

When the agent analyzes a repository, you'll see:
```
âœ“ Extracted 86 code symbols (17 classes, 69 functions)
Note: 80 test functions found via AST (pytest will find more including parametrized tests)
âœ“ Ran pytest --collect-only
```

Then the analysis will cite: **"204 tests collected [evidence: pytest --collect-only]"**

**Explanation:**
- **AST Extraction (80)**: Finds top-level `def test_*()` functions by parsing code syntax
- **pytest Collection (204)**: Finds ALL tests including:
  - Class test methods (`TestClass.test_method`)
  - Parametrized tests (`test_foo[param1]`, `test_foo[param2]`)
  - Dynamically generated tests
  
**Which number to use?** Always cite the **pytest count** (204) - it's the actual truth.

The AST count is useful for understanding code structure, but pytest is authoritative for test metrics.

### What Makes It Evidence-Based?

#### 1. Concrete Code References Required
âŒ **Before**: "This repository likely handles document processing..."  
âœ… **After**: "`DocumentProcessor` class in `src/processor.py` handles PDF extraction"

- **Demands**: Actual class names, function names in backticks
- **Example**: `` `AgentRunner`, `planning_node()`, `retrieval_node()` ``
- **Scoring**: 15 points for concrete code symbols

#### 2. Test File Citations
âŒ **Before**: "The system has comprehensive tests"  
âœ… **After**: "`tests/test_integration/test_workflow.py::test_full_cycle` validates end-to-end execution"

- **Demands**: Specific test files with `::test_function` syntax
- **Example**: `tests/test_agent/test_planning.py::test_route_selection`
- **Scoring**: 10 points for test citations

#### 3. Dependency Versions
âŒ **Before**: "Uses pytest for testing"  
âœ… **After**: "`pytest==7.4.3` - Test framework, used in 27 test files"

- **Demands**: Package versions from `requirements.txt`
- **Example**: `openai==1.3.5`, `azure-ai-formrecognizer==3.2.0`
- **Scoring**: 10 points for versions

#### 4. Real Metrics
âŒ **Before**: "Good test coverage"  
âœ… **After**: "85% coverage (450/530 lines covered)"

- **Demands**: Actual numbers from `.coverage` file
- **Example**: "27 test files, 107 tests, 94.8% pass rate"
- **Scoring**: 5 points for concrete metrics

#### 5. No Vague Language
âŒ **Prohibited**: "likely", "suggests", "appears to", "may", "probably", "seems to"  
âœ… **Required**: Definitive statements backed by evidence

- **Penalty**: -10 points for 5+ vague terms, -5 for 3+ terms
- **Enforcement**: Reflector critiques vague claims, Evaluator penalizes them

### How It Works

1. **Reflector Demands Evidence**
   ```
   Evidence Check (for repo analysis):
   - Are there specific file paths and class/function names cited?
   - Are dependencies listed with versions?
   - Are actual test files referenced?
   - Are claims grounded in code, or just assumptions?
   ```

2. **Generator Uses Evidence-Based Prompts**
   ```
   ğŸš¨ CRITICAL REQUIREMENTS - NO VAGUE CLAIMS:
   1. Cite actual code: file paths, class names, function names
   2. Name real tests: tests/test_x.py::test_feature
   3. Extract dependencies: From requirements.txt with versions
   4. Use real metrics: Coverage %, test counts
   5. No assumptions: Only claim what you can prove
   ```

3. **Evaluator Rewards Evidence**
   - Code symbols (classes/functions): +15 pts
   - Test citations: +10 pts
   - Dependency versions: +10 pts
   - Metrics: +5 pts
   - **Vague language: -10 pts**

### Real-World Example

**Query**: "Analyze this repository"

**Evidence-Based Output**:
```markdown
## Overview
Autonomous AI agent system built with LangGraph. Evidence:
- `AgentRunner` in src/agent/orchestrator.py::run_agent() orchestrates workflow
- `planning_node()` in src/agent/nodes/planner.py routes tasks
- Tests: tests/test_agent/test_orchestrator.py::test_workflow validates execution

## Dependencies (from requirements.txt)
1. langgraph==0.0.26 - State graph orchestration (imported in src/agent/orchestrator.py)
2. pytest==7.4.3 - Test framework (27 test files, 107 tests)
3. openai==1.3.5 - LLM calls via src/agent/nodes/reasoner.py::reasoning_node()

## System Capabilities
- **Agent Orchestration**: Implemented in orchestrator.py::run_agent()
  - Tested by: tests/test_agent/test_orchestrator.py::test_agent_flow
  - Dependencies: langgraph==0.0.26, azure-openai==1.3.5
  
## Quality Metrics
- Test Coverage: 85% (450/530 lines from .coverage)
- Test Files: 27 files in tests/ directory
- Pass Rate: 107/107 tests (100%)
```

**Score**: 90-95/100 (vs 70-80 for generic analysis)

---

## ğŸ“‹ Prerequisites

### v1.0 Requirements (Still Needed)
- Python 3.11+
- Azure OpenAI account
- Docker (optional)

### v2.0 Additional Requirements
- LangGraph 0.0.20+
- LangSmith 0.1.0+ (optional, for tracing)
- GitPython 3.1.41
- LibCST 1.1.0+

---

## ğŸš€ Quick Start - v2.0 Agent

### 1. Install Dependencies

```bash
# Activate virtual environment
source .venv/bin/activate

# Install v2.0 dependencies
pip install -r requirements.txt
```

### 2. Configure Environment

Update your `.env` file with v2.0 settings:

```env
# v1.0 Settings (keep these)
AZURE_OPENAI_API_KEY="your_key"
AZURE_OPENAI_ENDPOINT="your_endpoint"
OPENAI_API_VERSION="2023-12-01-preview"
EMBEDDING_MODEL_NAME="text-embedding-ada-002"
LLM_MODEL_NAME="gpt-4o"

# v2.0 Agent Settings (new)
AGENT_MAX_ITERATIONS=10
AGENT_TEMPERATURE=0.7
AGENT_ENABLE_REFLECTION=true
AGENT_ENABLE_TRACING=false
LANGSMITH_API_KEY=""
LANGSMITH_PROJECT="simple-rag-v2"
```

### 3. Run the Agent

#### Interactive CLI (Recommended for Demos)

```bash
# Start interactive conversation with the agent
python interactive_agent.py

# Or with verbose output (shows reasoning steps)
python interactive_agent.py --verbose
```

The interactive CLI allows you to:
- Have natural conversations with the agent
- Ask about repository structure
- Request code explanations
- Generate content (LinkedIn posts, etc.)
- See live evaluation scores

**Commands**: `exit`, `quit`, `history`, `stats`, `clear`, `help`

See [Interactive CLI Guide](docs/INTERACTIVE_CLI_GUIDE.md) for details.

#### Programmatic Usage

```python
from src.agent.orchestrator import run_agent

# Run agent on a task
result = await run_agent(
    task="Analyze the repository structure and dependencies",
    task_type="analyze_repo"
)

# Access results
print(f"Output: {result['final_output']}")
print(f"Score: {result['evaluation_scores']['overall_score']}")
```

#### Pre-built Demos

```bash
# Run full demo sequence (~5 min)
python demo_commands.py

# Quick test (30 sec)
python demo_commands.py --test

# Repository analysis only
python demo_commands.py --demo1

# LinkedIn post generation only
python demo_commands.py --demo2
```

---

## ğŸ¯ Usage Examples

### Example 1: Repository Analysis

```python
from src.agent.orchestrator import run_agent

result = await run_agent(
    task="Analyze this codebase and identify the main components",
    task_type="analyze_repo"
)

# Result includes:
# - Repository structure analysis
# - Dependency extraction
# - Architecture mapping
# - Reasoning steps
# - Evaluation scores
```

### Example 2: Question Answering

```python
result = await run_agent(
    task="What is RAG and how does it work?",
    task_type="answer_question"
)

# Agent will:
# 1. Retrieve relevant context from vector store
# 2. Reason about the information
# 3. Reflect on answer quality
# 4. Generate comprehensive response
```

### Example 3: Custom Task

```python
result = await run_agent(
    task="Explain the evaluation metrics used in this system",
    task_type="explain",
    max_iterations=5
)
```

---

## ğŸ“Š Evaluation Metrics (With Transparent Explanations!)

The v2.0 agent includes a comprehensive **task-aware** 5-metric evaluation system with detailed explanations showing **WHY** each score was earned.

### 1. Task Completion Score (0-100, Weight: 35%)
- Has final output: 50 points
- Marked as complete: 30 points
- Within iteration limit: 20 points

### 2. Reasoning Quality Score (0-100, Weight: 25%)
- Has reasoning steps: 40 points
- Quality based on depth (3-5 steps): up to 40 points
- Has reflection integration: 20 points

### 3. Tool Effectiveness Score (0-100, Weight: 15%)
- Used tools: 50 points
- Multiple diverse tools (3+): up to 30 points
- Tools produced useful results: 20 points

### 4. Reflection Quality Score (0-100, Weight: 10%)
- Has reflection notes: 30 points
- Depth of critique (3+ insights): up to 30 points
- **Reflection incorporated in output (PROOF)**: up to 40 points

### 5. Output Quality Score (0-100, Weight: 15%) - **TASK-AWARE**
#### For Code Questions:
- **Specificity (40 pts)**: File paths, line numbers, code excerpts, identifiers
- **Completeness (30 pts)**: Adequate length, context provided
- **Reflection Integration (30 pts)**: Has improvement section, references critique

#### For LinkedIn Posts:
- **Structure (30 pts)**: Opening, features, call-to-action
- **Content (40 pts)**: Hashtags, emojis, technical terms
- **Accuracy (30 pts)**: Uses repo data, demonstrates reflection

#### For Repository Analysis (EVIDENCE-BASED):
- **Completeness (35 pts)**: Required sections present (Overview, Architecture, Dependencies, Structure, Capabilities)
- **Evidence-Based Accuracy (40 pts)**: 
  - Concrete code symbols: Class/function names in backticks (15 pts)
  - Test file citations: `tests/test_x.py::test_feature` format (10 pts)
  - Dependency versions: `package==1.2.3` from requirements (10 pts)
  - Real metrics: Coverage %, counts, percentages (5 pts)
- **Structure (15 pts)**: Markdown formatting, lists
- **Self-Reflection (10 pts)**: Improvement section with evidence
- **Penalty (-10 pts)**: Vague language ("likely", "suggests", "appears to", "may", "probably")

### Overall Weighted Score
```
Overall = (
    Task Completion Ã— 0.35 +
    Reasoning Quality Ã— 0.25 +
    Tool Effectiveness Ã— 0.15 +
    Reflection Quality Ã— 0.10 +
    Output Quality Ã— 0.15
)
```

### Evaluation Explanations Example
```
ğŸ“‹ WHY THESE SCORES?

âœ¨ Output Quality (95.0/100):
  ğŸ“‹ Task Type: CODE QUESTION (specialized evaluation)
  
  Specificity Indicators (40 pts):
     âœ… File paths present: orchestrator.py (10/10 pts)
     âœ… Line numbers specified (10/10 pts)
     âœ… Code excerpts included (10/10 pts)
     âœ… Function/class names present (10/10 pts)
  
  Completeness (30 pts):
     âœ… Comprehensive answer (800 chars, 15/15 pts)
     âœ… Includes context/explanation (15/15 pts)
  
  Self-Reflection Integration (30 pts):
     âœ… Has 'ğŸ” How Self-Reflection Improved' section (15/15 pts)
     âœ… Strong critique references (15/15 pts)
```

---

## ğŸ§ª Testing

### Run All Tests

```bash
# Run complete test suite (107 tests)
pytest -v

# Run only v2.0 tests
pytest tests/test_agent/ tests/test_tools/ tests/test_evaluation/ -v

# Run integration tests
pytest tests/test_integration/ -v

# Run with coverage
pytest --cov=src --cov-report=term-missing
```

### Test Coverage

| Component | Tests | Status |
|-----------|-------|--------|
| Environment Setup | 17 | âœ… 100% |
| Project Structure | 22 | âœ… 100% |
| Agent State | 15 | âœ… 100% |
| Orchestrator | 14 | âœ… 100% |
| Repository Tools | 17 | âœ… 100% |
| Agent Nodes | 25 | âœ… 100% |
| Generation | 8 | âœ… 100% |
| Evaluation | 10 | âœ… 100% |
| Integration (E2E) | 18 | âœ… 100% |
| **Total** | **107** | **âœ… 100%** |

---

## ğŸ“ v2.0 Project Structure

```
Simple-RAG/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/                    # v2.0 Agent System
â”‚   â”‚   â”œâ”€â”€ state.py             # AgentState TypedDict (15+ fields)
â”‚   â”‚   â”œâ”€â”€ orchestrator.py      # LangGraph workflow
â”‚   â”‚   â””â”€â”€ nodes/               # Agent nodes
â”‚   â”‚       â”œâ”€â”€ planner.py       # Task planning & routing
â”‚   â”‚       â”œâ”€â”€ repo_analyzer.py # Repository analysis (4 tools)
â”‚   â”‚       â”œâ”€â”€ retriever.py     # RAG retrieval (NEW!)
â”‚   â”‚       â”œâ”€â”€ reasoner.py      # Chain-of-thought reasoning
â”‚   â”‚       â”œâ”€â”€ reflector.py     # Critical self-reflection
â”‚   â”‚       â”œâ”€â”€ generator.py     # BEFORE/AFTER generation
â”‚   â”‚       â””â”€â”€ evaluator.py     # Performance evaluation
â”‚   â”œâ”€â”€ tools/                   # v2.0 Tools
â”‚   â”‚   â”œâ”€â”€ repository_tools.py  # 4 repo analysis tools
â”‚   â”‚   â”œâ”€â”€ rag_tools.py         # RAG integration
â”‚   â”‚   â””â”€â”€ generation_tools.py  # Content generation
â”‚   â”œâ”€â”€ evaluation/              # v2.0 Evaluation (NEW!)
â”‚   â”‚   â”œâ”€â”€ metrics.py           # Task-aware 5 metrics
â”‚   â”‚   â”œâ”€â”€ evaluator.py         # AgentEvaluator class
â”‚   â”‚   â””â”€â”€ explanations.py      # Score explanations (NEW!)
â”‚   â”œâ”€â”€ config.py                # v1.0 Configuration
â”‚   â”œâ”€â”€ data_loader.py           # v1.0 Multi-modal processing
â”‚   â”œâ”€â”€ text_processor.py        # v1.0 Text chunking
â”‚   â”œâ”€â”€ vector_store.py          # v1.0 ChromaDB
â”‚   â””â”€â”€ chatbot.py               # v1.0 RAG pipeline
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_agent/              # Agent tests
â”‚   â”‚   â”œâ”€â”€ test_state.py
â”‚   â”‚   â”œâ”€â”€ test_orchestrator.py
â”‚   â”‚   â””â”€â”€ test_nodes/
â”‚   â”œâ”€â”€ test_tools/              # Tool tests
â”‚   â”œâ”€â”€ test_evaluation/         # Evaluation tests
â”‚   â”œâ”€â”€ test_integration/        # E2E tests
â”‚   â””â”€â”€ [v1.0 tests...]
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ v2.0/                    # v2.0 Documentation
â”‚       â”œâ”€â”€ 00-EXECUTIVE-SUMMARY.md
â”‚       â”œâ”€â”€ 01-PROJECT-OVERVIEW.md
â”‚       â”œâ”€â”€ 02-TECHNICAL-ARCHITECTURE.md
â”‚       â”œâ”€â”€ 03-IMPLEMENTATION-GUIDE.md
â”‚       â”œâ”€â”€ 04-TESTING-STRATEGY.md
â”‚       â”œâ”€â”€ 05-FINAL-RECOMMENDATIONS.md
â”‚       â”œâ”€â”€ AGENT_CONTEXT_PROMPT.md
â”‚       â”œâ”€â”€ PROGRESS_TRACKING.md
â”‚       â”œâ”€â”€ BASELINE_VERIFICATION_REPORT.md
â”‚       â”œâ”€â”€ SPRINT_1_2_SUMMARY.md
â”‚       â””â”€â”€ SPRINT_3_4_SUMMARY.md
â”œâ”€â”€ main.py                      # v1.0 CLI entry point
â”œâ”€â”€ requirements.txt             # All dependencies
â””â”€â”€ README_V2.md                 # This file
```

---

## ğŸ”§ Configuration

### Agent Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| `AGENT_MAX_ITERATIONS` | Maximum agent iterations | `10` |
| `AGENT_TEMPERATURE` | LLM temperature for agent | `0.7` |
| `AGENT_ENABLE_REFLECTION` | Enable self-reflection | `true` |
| `AGENT_ENABLE_TRACING` | Enable LangSmith tracing | `false` |
| `LANGSMITH_API_KEY` | LangSmith API key (optional) | `""` |
| `LANGSMITH_PROJECT` | LangSmith project name | `"simple-rag-v2"` |

---

## ğŸ“ Key Concepts

### Agent State
The core data structure that flows through the agent (15+ tracked fields):

```python
class AgentState(TypedDict):
    # Core task
    task: str
    task_type: str  # analyze_repo/answer_question/generate_content/general
    messages: Annotated[Sequence[BaseMessage], operator.add]
    
    # Repository analysis results
    repo_structure: Optional[Dict]
    code_files: Optional[List[Dict]]
    dependencies: Optional[Dict]
    architecture: Optional[Dict]
    
    # RAG context (NEW!)
    retrieved_context: Optional[List[Dict]]  # Chunks from ChromaDB
    
    # Reasoning trail (transparency)
    reasoning_steps: Annotated[List[str], operator.add]
    reflection_notes: Annotated[List[str], operator.add]
    reflection_assessment: Optional[str]  # "good" or "needs_improvement" (NEW!)
    tool_usage: Annotated[List[Dict], operator.add]
    
    # Generation (with demo feature)
    draft_content: Optional[str]
    output_before_reflection: Optional[str]  # Baseline (NEW!)
    final_output: Optional[str]
    
    # Evaluation (with explanations)
    evaluation_scores: Optional[Dict[str, float]]
    evaluation_explanations: Optional[Dict[str, List[str]]]  # NEW!
    
    # Control flow
    next_action: str
    iteration_count: int
    max_iterations: int
    is_complete: bool
```

### LangGraph Workflow
State-based orchestration with conditional routing (6 nodes):

```python
# Create graph
graph = StateGraph(AgentState)

# Add nodes
graph.add_node("planner", planning_node)
graph.add_node("repo_analyzer", repo_analyzer_node)
graph.add_node("retriever", retrieval_node)  # NEW! RAG retrieval
graph.add_node("reasoner", reasoning_node)
graph.add_node("reflector", reflection_node)
graph.add_node("generator", generation_node)
graph.add_node("evaluator", evaluation_node)

# Add conditional edges for routing
graph.add_conditional_edges(
    "planner",
    route_after_planning,
    {
        "analyze": "repo_analyzer",
        "retrieve": "retriever",  # NEW! Routes to ChromaDB
        "reason": "reasoner",
        "end": END
    }
)

graph.add_conditional_edges("reflector", route_after_reflection)

# Compile
agent = graph.compile()
```

---

## Documentation

### Complete Documentation Set

#### Evidence-Based Analysis
- **[Evidence-Based Analysis System](./docs/v2.0/EVIDENCE_BASED_ANALYSIS.md)** - Complete guide to AST extraction, verification commands, and evidence tags
  - How code symbols are extracted
  - Verification commands (pytest, coverage, find)
  - Evidence tag requirements
  - Before/After examples
  - CEO requirements compliance

#### Self-Reflection & Critique
- **[Self-Reflection System](./docs/v2.0/SELF_REFLECTION_SYSTEM.md)** - Critical self-assessment mechanism
  - How reflection works
  - Critique criteria by task type
  - BEFORE/AFTER generation
  - Evaluation scoring

#### RAG System
- **[RAG (Retrieval-Augmented Generation)](./docs/v2.0/RAG_SYSTEM.md)** - Knowledge base integration
  - ChromaDB setup and configuration
  - Vector embeddings (OpenAI ada-002)
  - Ingestion strategies
  - Retrieval optimization
  - Query examples

#### LangGraph Orchestration
- **[LangGraph Workflow Orchestration](./docs/v2.0/LANGGRAPH_ORCHESTRATION.md)** - State graph framework
  - Agent state structure
  - Node definitions
  - Conditional routing
  - Iteration control
  - Execution flow examples
  - Debugging tips

### Architecture & Design
- **[Architecture Diagram](./docs/v2.0/architecture-v2.mmd)** - Visual Mermaid workflow
- **[Agent Flow Documentation](./AGENT_FLOW_DOCUMENTATION.md)** - Detailed node descriptions
- **[BEFORE/AFTER Demo](./BEFORE_AFTER_REFLECTION_DEMO.md)** - Real reflection examples

### Verification & Testing
- **[CEO Evidence Verification](./CEO_EVIDENCE_ONLY_VERIFICATION.md)** - Evidence-only compliance report
  - CEO requirements
  - Test results
  - Before/After comparison
  - Acceptance criteria
- **[Evidence Quality Tests](./test_ceo_evidence.py)** - Automated verification script
- **[Real Evidence Tests](./test_real_evidence.py)** - Symbol extraction tests

### Bug Fixes & Improvements
- **[Bug Fix: Excessive Reasoning Steps](./BUG_FIX_EXCESSIVE_REASONING_STEPS.md)**
- **[Bug Fix: Code Questions](./BUG_FIX_CODE_QUESTIONS.md)**
- **[Improvement: Evidence-Based Analysis](./IMPROVEMENT_EVIDENCE_BASED_ANALYSIS.md)**

### v1.0 Documentation
- **[v1.0 README](./README.md)** - Original RAG chatbot documentation
- **[v1.0 Architecture](./ARCHITECTURE.md)** - Simple RAG system design of complete system

#### Feature-Specific Guides
- **`BEFORE_AFTER_REFLECTION_DEMO.md`** (NEW!): Complete guide for self-reflection demonstration feature
- **`EVALUATION_EXPLANATIONS_FEATURE.md`** (NEW!): Guide for transparent evaluation system
- **`CIKLUM_DEMO_SEQUENCES.md`**: Exact demo sequences for Ciklum presentation

#### Technical Documentation
- **Executive Summary**: High-level overview
- **Project Overview**: Goals and requirements
- **Technical Architecture**: Detailed system design
- **Implementation Guide**: Step-by-step development
- **Testing Strategy**: Comprehensive test approach
- **Agent Context**: Development guidelines
- **Progress Tracking**: Task completion status
- **Sprint Summaries**: Detailed sprint reports

All documentation available in `docs/v2.0/` and root directory

---

## ğŸš€ Development Workflow

### TDD Approach
All v2.0 features were built using Test-Driven Development:

1. **Write tests first** (RED)
2. **Implement minimal code** (GREEN)
3. **Refactor and improve** (REFACTOR)
4. **Document thoroughly**
5. **Commit with descriptive messages**

### Sprint Completion
- âœ… Sprint 0: Environment Setup (3/3 tasks)
- âœ… Sprint 1: Agent Framework (2/2 tasks)
- âœ… Sprint 2: Repository Tools (4/4 tasks)
- âœ… Sprint 3: Intelligent Nodes (3/3 tasks)
- âœ… Sprint 4: Evaluation (2/2 tasks)
- âœ… Sprint 5: Documentation (3/3 tasks)

**Total**: 15/15 tasks (100%)

---

## ğŸ” Backward Compatibility

v2.0 is fully backward compatible with v1.0:

```python
# v1.0 RAG still works
from src.chatbot import RAGChatbot

chatbot = RAGChatbot()
answer = chatbot.ask("What is RAG?")

# v2.0 agent adds new capabilities
from src.agent.orchestrator import run_agent

result = await run_agent("What is RAG?", "answer_question")
```

---

## ğŸ¤ Contributing

### Code Quality Standards
- âœ… TDD methodology
- âœ… Type hints throughout
- âœ… Comprehensive documentation
- âœ… 100% test coverage for new code
- âœ… Production-ready error handling

### Development Setup
```bash
# Clone and setup
git clone <repo>
cd Simple-RAG

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run tests
pytest -v
```

---

## ğŸ“ˆ Performance

### Test Execution
- **Total Tests**: 107
- **Execution Time**: ~1.2 seconds
- **Success Rate**: 100%

### Agent Performance
- **Average Iterations**: 2-4 per task
- **Completion Rate**: 100%
- **Average Score**: 75-90 (task-dependent)

---

## ğŸ› Troubleshooting

### v2.0 Specific Issues

**"Cannot import AgentState"**
- Ensure v2.0 dependencies installed: `pip install -r requirements.txt`
- Check Python version: `python --version` (need 3.11+)

**"LangGraph not found"**
- Install LangGraph: `pip install langgraph>=0.0.20`

**Agent not completing**
- Check `AGENT_MAX_ITERATIONS` setting
- Review agent logs for errors
- Verify all nodes are functioning

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ™ Acknowledgments

### v2.0 Development
- Built following TDD best practices
- Implements LangGraph patterns
- Uses production-quality evaluation
- Comprehensive test coverage

### Technologies
- **LangGraph**: Agent orchestration
- **LangChain**: LLM integration
- **LangSmith**: Tracing (optional)
- **GitPython**: Repository analysis
- **LibCST**: Code parsing
- **ChromaDB**: Vector storage (v1.0)
- **Azure OpenAI**: LLM & embeddings (v1.0)

---

## ğŸ“§ Contact

For questions about v2.0 implementation:
- Review documentation in `docs/v2.0/`
- Check test examples in `tests/`
- Create an issue in the repository

---

**v2.0 Built with Production Quality Standards** âœ¨

**Test Coverage**: 107/107 tests passing (100%)  
**Code Quality**: TDD, Type Hints, Documentation  
**Status**: Production Ready âœ…
