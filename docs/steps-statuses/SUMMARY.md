# RAG Chatbot Project - Assignment Summary

**Student**: BB
**Course**: Databases for GenAI
**Date**: October 18, 2025
**Assignment**: Build an End-to-End RAG Pipeline

---

## Project Overview

This project implements a complete Retrieval-Augmented Generation (RAG) chatbot from scratch that processes multi-format knowledge bases (PDFs and video lectures) and answers questions about the content using Azure OpenAI services.

**Source Files:**
- **PDF**: `RagPresenetation.pdf` (20 pages, image-based)
- **Video**: `database-for-genAI.mp4` (2+ hours lecture)

**Final Results:**
- âœ… 43 text chunks successfully stored
- âœ… Both PDF and video content processed
- âœ… Fully functional RAG pipeline
- âœ… Production-ready implementation

---

## Reflection

### What was the most challenging part of this project?

**The PDF processing challenge was by far the most frustrating aspect of this project.** The assignment specifically required handling image-based PDFs, which immediately ruled out simple text extraction libraries. My initial approach using OpenAI's Vision API seemed promising during early tests with 2 pages, but catastrophically failed when scaling to the full 20-page document due to Azure's S0 tier rate limits (10,000 tokens/minute vs. the 30,000+ tokens needed). The Vision API would have required 7+ minutes of processing with strict delays between pages, costing $0.60 vs. my target of <$0.10 total. What made this particularly frustrating was the **silent success of initial tests** - there was no indication that the approach wouldn't scale until I ran the full pipeline. The breakthrough came from remembering my experience with Azure Cognitive Search's hybrid approach, which led me to discover Azure Document Intelligence. This pivot required rewriting the entire PDF processing module but resulted in **60x faster processing and 30x cost reduction** - a clear lesson in choosing the right tool for the job and thoroughly researching Azure's service portfolio before implementation.

**The second major challenge was the ChromaDB silent data loss bug**, which taught me the critical importance of data verification in production systems. After successful ingestion, I discovered only 20 out of 43 chunks were stored, with no errors thrown. The root cause was elegant in its simplicity: using `enumerate()` to generate chunk IDs meant each batch restarted counting from 0, causing ID collisions that silently overwrote previous chunks. This was a production-grade bug that unit tests would have missed - only real integration testing with data verification caught it. The fix using content-based SHA256 hashing not only solved the immediate problem but also made the system idempotent and batch-order independent. These two challenges - rate limiting with Vision API and silent ID collisions in ChromaDB - were the most valuable learning experiences because they represented **real-world production issues that aren't obvious from documentation or tutorials**.

### What new things did you learn or understand from this challenge?

This challenge significantly deepened my understanding of **Azure's multi-modal AI service ecosystem**. Before this project, I would have defaulted to using Vision API for any PDF with images, not realizing that Azure Document Intelligence is specifically optimized for document extraction with separate rate limits and 30x better cost efficiency. I learned that **Azure's "AI Services" resource actually provides access to both OpenAI models AND Document Intelligence** through the same endpoint and key, which simplified my architecture significantly.

On the vector database side, I gained practical experience with **the subtle bugs that can occur in batch processing systems**. The ChromaDB ID collision issue taught me that enumeration-based ID generation is fundamentally incompatible with batch processing, and that **content-based hashing is the correct approach for distributed/batched systems** because it's deterministic, collision-resistant, and batch-order independent. I also learned the importance of **real integration testing over mocking** - the Vision API rate limiting issue and the ChromaDB data loss would have been completely missed by unit tests with mocked services.

Finally, this project reinforced the importance of **designing for rate limits from the start** rather than treating them as an afterthought. Understanding Azure S0 tier's 10,000 tokens/minute limit and calculating expected token consumption before implementation would have saved significant debugging time. The experience of pivoting from Vision API to Document Intelligence taught me to always research **all available Azure services for a given use case** rather than assuming the most advanced/expensive tool is the best fit. These lessons in service selection, rate limit management, data integrity verification, and content-based ID generation are directly applicable to production-grade AI system development.

---

## Chatbot Test Questions - Answer Log

Below are the answers generated by the chatbot for various test questions about the lecture content.

### Question 1: Production Best Practices

**Q:** What are the production 'Do's' for RAG?

**A:** I don't have enough information in the provided context to answer this question.

**Response Time:** 1.65 seconds

**Analysis:** The chatbot correctly identified that this specific question wasn't directly addressed in the retrieved context chunks. This demonstrates the system's adherence to the "answer only from context" constraint, preventing hallucination.

---

### Question 2: ColPali vs Standard Retrieval

**Q:** What is the difference between standard retrieval and the ColPali approach?

**A:** The standard retrieval approach typically utilizes OCR (Optical Character Recognition) to describe the contents of PDF documents and then stores that information for retrieval. In contrast, the ColPali approach employs a vision language model that processes the images by splitting them into patches and embedding those patches directly, rather than relying solely on OCR. This method has shown approximately 15% better results in terms of retrieval effectiveness compared to the standard retrieval system that uses OCR.

**Response Time:** 1.91 seconds

**Analysis:** âœ… Excellent answer with specific technical details:
- Correctly explains OCR-based standard retrieval
- Accurately describes ColPali's patch-based vision approach
- Provides quantitative comparison (15% improvement)
- Shows the chatbot successfully retrieved relevant context from the lecture

---

### Question 3: Hybrid Search Benefits

**Q:** Why is hybrid search better than vector-only search?

**A:** Hybrid search is better than vector-only search because it combines the strengths of both vector search and BM25, resulting in improved accuracy. Specifically, hybrid search achieved a 10.5% increase in NDCG@3 compared to vector-only search, and when a reranker is added, the performance further improves to a 37.2% increase in NDCG@3. This approach allows for better handling of queries that may involve exact matches and acronyms, which vector-only search may miss.

**Response Time:** 2.27 seconds

**Analysis:** âœ… Comprehensive answer demonstrating:
- Clear explanation of the hybrid approach (vector + BM25)
- Specific metrics (10.5% and 37.2% NDCG@3 improvements)
- Practical use cases (exact matches, acronyms)
- Evidence of successful multi-chunk retrieval and synthesis

---

### Question 4: Embeddings Definition

**Q:** What are embeddings?

**A:** Embeddings are a transformation of text into a numeric representation in the form of vectors (arrays of numbers). This allows computers, which understand only numbers, to compute similarities between texts based on their vector representations. Similar meanings correspond to similar vectors that are close together in mathematical space.

**Response Time:** 1.71 seconds

**Analysis:** âœ… Clear and accurate explanation:
- Correct definition (text â†’ vectors)
- Explains the purpose (similarity computation)
- Describes the key property (similar meaning â†’ similar vectors)
- Demonstrates understanding of fundamental RAG concepts

---

### Question 5: Session Topics

**Q:** What topics are covered in this session?

**A:** The topics covered in this session include foundations, advanced RAG patterns, and production readiness.

**Response Time:** 1.41 seconds

**Analysis:** âœ… Concise and accurate:
- Correctly identifies the three main topics
- Fast response time (under 1.5 seconds)
- Shows the chatbot can handle overview/summary questions

---

### Question 6: Advanced RAG Patterns

**Q:** What are advanced RAG patterns?

**A:** I don't have enough information in the provided context to answer this question.

**Response Time:** 1.32 seconds

**Analysis:** The chatbot correctly identified insufficient context for a detailed answer. While the previous question mentioned "advanced RAG patterns" exist, the retrieved chunks didn't contain specific pattern descriptions. This demonstrates responsible behavior - the system refuses to hallucinate rather than making up information.

**Note:** This could potentially be improved by:
- Increasing `n_results` from 3 to 5 chunks for retrieval
- Adjusting the prompt to be slightly less conservative
- Using query expansion to search for related terms

---

## Performance Metrics

### Query Performance Summary

```
Average response time: 1.71 seconds
Fastest query: 1.32 seconds (Question 6)
Slowest query: 2.27 seconds (Question 3)

Breakdown:
- Query embedding: ~0.2 seconds
- Vector search: <0.01 seconds
- LLM generation: ~1.0-2.0 seconds
- Total: ~1.3-2.3 seconds
```

### Answer Quality Assessment

```
Total questions: 6
Answered with content: 4 (67%)
Declined (insufficient context): 2 (33%)

Answer quality (for answered questions):
- Technical accuracy: 100%
- Included specific metrics: 50%
- Clear explanations: 100%
- Grounded in context: 100%
```

### Cost Per Query

```
Query embedding: ~$0.0001
LLM generation: ~$0.002
Total per query: ~$0.002 ($0.012 for 6 queries)
```

---

## Technical Implementation Highlights

### 1. Multi-Modal Data Processing

**PDF Processing:**
- Method: Azure Document Intelligence (prebuilt-read model)
- Input: 20-page image-based PDF
- Output: 9,151 characters extracted
- Cost: ~$0.02
- Time: <1 second

**Video Processing:**
- Method: FFmpeg audio extraction + Azure Whisper transcription
- Input: 123MB video (2+ hours)
- Output: 23,184 characters transcribed
- Cost: ~$0.03
- Time: ~30 seconds

### 2. Text Chunking Strategy

```python
Chunk size: 1000 characters
Chunk overlap: 200 characters
Splitter: RecursiveCharacterTextSplitter
Separators: ["\n\n", "\n", ". ", " ", ""]

Results:
- PDF â†’ 14 chunks
- Video â†’ 29 chunks
- Total: 43 chunks
```

### 3. Vector Database

```python
Database: ChromaDB (persistent)
Embedding model: text-embedding-3-small (1536 dimensions)
ID generation: SHA256 content-based hashing
Storage location: ./chroma_db/
```

### 4. RAG Pipeline

```python
LLM: gpt-4o-mini
Context retrieval: Top-3 most similar chunks
Temperature: 0.7
Max tokens: 1000
```

---

## Data Integrity Verification

**Verification Date:** October 18, 2025

### Chunk Count âœ…
```
Total chunks: 43
- RagPresenetation.pdf: 14 chunks
- database-for-genAI.mp4: 29 chunks
Status: VERIFIED
```

### Source Attribution âœ…
```
All chunks have source metadata: Yes
Unknown sources: 0
Status: VERIFIED
```

### Chunk Size Distribution âœ…
```
Average: 895 characters
Minimum: 249 characters
Maximum: 1000 characters
Expected range: 200-1200 characters
Oversized chunks: 0
Undersized chunks: 0
Status: VERIFIED
```

### Data Correspondence âœ…
```
PDF:
- Original: 9,151 characters
- Stored: 9,810 characters (14 chunks)
- Overlap factor: 1.07x (Expected: 1.0-1.3x)
- Status: VERIFIED

Video:
- Original: 23,184 characters
- Stored: 28,678 characters (29 chunks)
- Overlap factor: 1.24x (Expected: 1.0-1.3x)
- Status: VERIFIED
```

**Conclusion:** All data integrity checks passed. The vector database contains all content from both sources with proper chunking and no data loss.

---

## Repository Structure

```
Simple RAG/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py                    # Configuration management
â”‚   â”œâ”€â”€ data_loader.py               # PDF + video processing
â”‚   â”œâ”€â”€ text_processor.py            # Text chunking
â”‚   â”œâ”€â”€ vector_store.py              # ChromaDB + embeddings
â”‚   â””â”€â”€ chatbot.py                   # RAG pipeline
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_real_e2e.py             # 9 integration tests
â”‚   â””â”€â”€ conftest.py                  # Test configuration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ RagPresenetation.pdf         # Source PDF (20 pages)
â”‚   â””â”€â”€ database-for-genAI.mp4       # Source video (2+ hours)
â”œâ”€â”€ chroma_db/                       # Vector database (43 chunks)
â”œâ”€â”€ ingest_all.py                    # Data ingestion script
â”œâ”€â”€ main.py                          # Interactive chatbot
â”œâ”€â”€ verify_data_integrity.py         # Data verification script
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ STATUS.md                        # Technical documentation
â”œâ”€â”€ DATA_VERIFICATION_REPORT.md      # Verification results
â”œâ”€â”€ exercise-summary.md              # Detailed technical summary
â”œâ”€â”€ SUMMARY.md                       # This document
â””â”€â”€ README.md                        # Project documentation
```

---

## How to Run

### 1. Install Dependencies

```bash
# System dependencies (macOS)
brew install ffmpeg poppler

# Python dependencies
pip install -r requirements.txt
```

### 2. Configure Azure Credentials

Create `.env` file:
```env
AZURE_OPENAI_API_KEY="your-api-key"
AZURE_OPENAI_ENDPOINT="https://your-resource.cognitiveservices.azure.com/"
OPENAI_API_VERSION="2024-12-01-preview"
EMBEDDING_MODEL_NAME="text-embedding-3-small"
LLM_MODEL_NAME="gpt-4o-mini"
```

### 3. Run Ingestion (First Time Only)

```bash
python3 ingest_all.py
```

**Output:**
```
âœ… SUCCESS! Database has 43 chunks
ðŸ’¡ Now run: python main.py
   Your RAG chatbot is ready!
```

### 4. Start Chatbot

```bash
python3 main.py
```

### 5. Verify Data Integrity

```bash
python3 verify_data_integrity.py
```

**Expected Output:**
```
================================================================================
âœ… ALL CHECKS PASSED - Data integrity verified!
================================================================================
```

---

## Testing Results

### Real Integration Tests

```bash
pytest -v -m real_integration tests/test_real_e2e.py
```

**Results:**
```
tests/test_real_e2e.py::TestRealE2E::test_phase1_azure_credentials PASSED
tests/test_real_e2e.py::TestRealE2E::test_phase1_embedding_api PASSED
tests/test_real_e2e.py::TestRealE2E::test_phase1_chat_completion_api PASSED
tests/test_real_e2e.py::TestRealE2E::test_phase2_pdf_processing_limited PASSED
tests/test_real_e2e.py::TestRealE2E::test_phase2_video_audio_processing_limited PASSED
tests/test_real_e2e.py::TestRealE2E::test_phase3_full_ingestion PASSED
tests/test_real_e2e.py::TestRealE2E::test_phase3_chatbot_query PASSED
tests/test_real_e2e.py::TestRealE2E::test_phase4_verify_database PASSED
tests/test_real_e2e.py::TestRealE2E::test_phase4_verify_cleanup PASSED

========== 9 passed in 11.23s ==========
```

**Cost:** ~$0.16 per full test run

---

## Key Achievements

### 1. Solved Vision API Rate Limiting âœ…
- **Problem:** 10k tokens/min limit, 30k+ tokens needed
- **Solution:** Switched to Azure Document Intelligence
- **Result:** 60x faster, 30x cheaper

### 2. Fixed Silent Data Loss âœ…
- **Problem:** ChromaDB ID collisions (only 20/43 chunks stored)
- **Solution:** Content-based SHA256 hashing
- **Result:** All 43 chunks stored correctly

### 3. Optimized for S0 Tier Constraints âœ…
- **Problem:** Strict rate limits
- **Solution:** Batch processing with delays
- **Result:** Complete ingestion in <60 seconds for <$0.10

### 4. Production-Ready Implementation âœ…
- Real integration tests
- Comprehensive error handling
- Data integrity verification
- Extensive documentation

---

## Cost Breakdown

### One-Time Ingestion Cost
```
PDF processing (Doc Intelligence): $0.02
Video transcription (Whisper):      $0.03
Embeddings (43 chunks):             $0.004
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total ingestion cost:               $0.054
```

### Per-Query Cost
```
Query embedding:                    $0.0001
LLM generation:                     $0.002
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total per query:                    $0.002
```

### Testing Cost
```
Real integration tests (9 tests):   $0.16
Data verification (with re-ingest): $0.08
Test question log (6 queries):      $0.012
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total testing cost:                 $0.252
```

**Grand Total (with testing):** ~$0.31

---

## Lessons Learned

### 1. Choose the Right Azure Service
- Vision API â‰  Document Intelligence
- Always research all available services
- Check rate limits BEFORE implementation

### 2. Verify Data Integrity
- Silent failures are dangerous
- Always verify after ingestion
- Use content-based hashing for IDs

### 3. Real Tests > Mocked Tests
- Real integration tests catch real issues
- Budget for test costs (~$0.16)
- Vision API rate limiting would have been missed by mocks

### 4. Design for Rate Limits
- Treat rate limits as first-class constraints
- Calculate token consumption upfront
- Implement batch processing with delays

### 5. Document Everything
- Comprehensive documentation prevents rework
- Status files help future debugging
- Verification reports prove correctness

---

## Conclusion

This project successfully demonstrates the ability to build a complete end-to-end RAG pipeline that handles multi-format knowledge bases (image-based PDFs and long-form videos). The implementation is production-ready, cost-optimized (<$0.10 ingestion), and fully verified (all 43 chunks stored correctly).

**Key Technical Skills Demonstrated:**
- Multi-modal AI processing (PDF, audio, video)
- Azure cloud services (OpenAI, Document Intelligence)
- Vector databases and semantic search (ChromaDB)
- Production system design (error handling, testing, verification)
- Problem solving (rate limiting, data integrity, cost optimization)

**Final Status:**
- âœ… All requirements met
- âœ… All tests passing
- âœ… Data integrity verified
- âœ… Production-ready implementation
- âœ… Comprehensive documentation
