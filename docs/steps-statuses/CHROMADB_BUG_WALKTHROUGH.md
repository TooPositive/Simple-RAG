# ChromaDB Silent Data Loss Bug - Complete Walkthrough

**Issue**: ID collisions causing silent data overwrites during batch processing
**Impact**: Only 20/43 chunks stored (53% data loss!)
**Severity**: Critical - Silent failure with no error messages
**Status**: ‚úÖ Fixed

---

## Table of Contents
1. [The Bug - Original Code](#the-bug---original-code)
2. [How We Discovered It](#how-we-discovered-it)
3. [Investigation Process](#investigation-process)
4. [The Root Cause](#the-root-cause)
5. [The Fix](#the-fix)
6. [Verification](#verification)
7. [Lessons Learned](#lessons-learned)

---

## The Bug - Original Code

### Original Implementation (BUGGY)

**File**: `src/vector_store.py` (lines 75-85, before fix)

```python
def embed_and_store_chunks(chunks: List[Dict[str, str]], collection: Collection) -> None:
    """
    Generates embeddings for text chunks and stores them in ChromaDB.
    """
    if not chunks:
        print("No chunks to embed. Skipping.")
        return

    print(f"Embedding and storing {len(chunks)} chunks...")

    # Initialize Azure OpenAI client
    client = AzureOpenAI(...)

    # Prepare data for ChromaDB
    documents_to_add = []
    metadatas_to_add = []
    ids_to_add = []

    # ‚ö†Ô∏è BUG IS HERE ‚ö†Ô∏è
    for i, chunk in enumerate(chunks):  # enumerate starts at 0 for EACH batch!
        documents_to_add.append(chunk["content"])
        metadatas_to_add.append({"source": chunk["source"]})
        ids_to_add.append(f"chunk_{i}")  # ‚ùå PROBLEM: i resets in each batch!

    # Generate embeddings and store
    response = client.embeddings.create(input=documents_to_add, model=...)
    embeddings = [item.embedding for item in response.data]

    collection.add(
        embeddings=embeddings,
        documents=documents_to_add,
        metadatas=metadatas_to_add,
        ids=ids_to_add  # ‚ùå IDs like: chunk_0, chunk_1, ..., chunk_19
    )
```

### How It Was Called (Batching)

**File**: `ingest_all.py` (lines 95-105)

```python
# Process in batches with delays (for rate limiting)
batch_size = 20
for i in range(0, len(chunks), batch_size):
    batch = chunks[i:i+batch_size]  # Slice chunks into batches
    print(f"Processing batch {i//batch_size + 1}...")

    embed_and_store_chunks(batch, collection)  # ‚ùå enumerate() resets here!

    if i + batch_size < len(chunks):
        time.sleep(2)  # Delay between batches
```

---

## How We Discovered It

### Symptom 1: Unexpected Chunk Count

After running `ingest_all.py` successfully (no errors!), we checked the database:

```bash
$ python3 -c "from src.vector_store import get_vector_database_collection; print(f'Chunks: {get_vector_database_collection().count()}')"

Chunks: 20  # ‚ö†Ô∏è Expected 43!
```

**Expected**: 43 chunks (9 from PDF + 29 from video + 5 continuation)
**Actual**: 20 chunks
**Loss**: 23 chunks missing (53% data loss!)

### Symptom 2: No Error Messages

The ingestion script showed:

```bash
================================================================================
COMPLETE DATA INGESTION
================================================================================

üìÑ Processing PDF with Document Intelligence...
‚úÖ PDF processed: 9151 characters

üé¨ Processing video...
   Extracting & compressing audio...
   Transcribing with Whisper...
‚úÖ Video processed: 23184 characters

================================================================================
CHUNKING & EMBEDDING
================================================================================

üìù Chunking 2 documents...
‚úÖ Created 43 chunks

üóÑÔ∏è  Storing in database...
   Processing batch 1/3 (20 chunks)...
‚úì Successfully embedded and stored 20 chunks in vector database
   Processing batch 2/3 (20 chunks)...
‚úì Successfully embedded and stored 20 chunks in vector database  # ‚ö†Ô∏è Looks fine!
   Processing batch 3/3 (3 chunks)...
‚úì Successfully embedded and stored 3 chunks in vector database   # ‚ö†Ô∏è Looks fine!

================================================================================
‚úÖ SUCCESS! Database has 20 chunks  # ‚ùå WAIT, should be 43!
================================================================================
```

**Red flags:**
- ‚úÖ All batch operations succeeded
- ‚úÖ No error messages
- ‚ùå Final count doesn't match (20 instead of 43)

---

## Investigation Process

### Step 1: Manual Database Inspection

```python
# test_inspection.py
from src.vector_store import get_vector_database_collection

collection = get_vector_database_collection()

# Get all IDs
results = collection.get()
ids = results['ids']

print(f"Total chunks: {len(ids)}")
print(f"\nAll IDs:")
for id in sorted(ids):
    print(f"  {id}")
```

**Output:**

```bash
$ python3 test_inspection.py

Total chunks: 20

All IDs:
  chunk_0
  chunk_1
  chunk_2
  chunk_3
  chunk_4
  chunk_5
  chunk_6
  chunk_7
  chunk_8
  chunk_9
  chunk_10
  chunk_11
  chunk_12
  chunk_13
  chunk_14
  chunk_15
  chunk_16
  chunk_17
  chunk_18
  chunk_19
```

**Discovery:** Only IDs 0-19 exist! IDs 20-42 are missing!

### Step 2: Check Source Distribution

```python
# Check which sources the 20 chunks came from
metadatas = results['metadatas']
source_counts = {}
for metadata in metadatas:
    source = metadata.get('source', 'UNKNOWN')
    source_counts[source] = source_counts.get(source, 0) + 1

print(f"\nChunks per source:")
for source, count in source_counts.items():
    print(f"  {source}: {count} chunks")
```

**Output:**

```bash
Chunks per source:
  database-for-genAI.mp4: 17 chunks  # ‚ö†Ô∏è Should be 29!
  RagPresenetation.pdf: 3 chunks     # ‚ö†Ô∏è Should be 14!
```

**Discovery:** Both sources are incomplete! Chunks from the end of processing are present, but earlier chunks are missing.

### Step 3: The "Aha!" Moment

Looking at the batch processing:

```
Batch 1 (chunks 0-19):   Creates IDs: chunk_0 to chunk_19   ‚úì
Batch 2 (chunks 20-39):  Creates IDs: chunk_0 to chunk_19   ‚ùå COLLISION!
Batch 3 (chunks 40-42):  Creates IDs: chunk_0 to chunk_2    ‚ùå COLLISION!
```

**ChromaDB behavior with duplicate IDs:**
- When you add a document with an existing ID, **it overwrites the old one**
- No error is thrown - it's treated as an "upsert" (update or insert)
- This is by design for ChromaDB, but deadly if you don't know about it!

---

## The Root Cause

### Why enumerate() Resets

```python
# Batch 1: chunks[0:20]
for i, chunk in enumerate(chunks[0:20]):
    # i goes: 0, 1, 2, ..., 19
    ids_to_add.append(f"chunk_{i}")  # Creates: chunk_0, chunk_1, ..., chunk_19

# Batch 2: chunks[20:40]
for i, chunk in enumerate(chunks[20:40]):
    # i goes: 0, 1, 2, ..., 19  ‚ùå RESETS TO 0!
    ids_to_add.append(f"chunk_{i}")  # Creates: chunk_0, chunk_1, ..., chunk_19 again!

# Batch 3: chunks[40:43]
for i, chunk in enumerate(chunks[40:43]):
    # i goes: 0, 1, 2  ‚ùå RESETS TO 0!
    ids_to_add.append(f"chunk_{i}")  # Creates: chunk_0, chunk_1, chunk_2 again!
```

### Visual Representation

```
Database state after each batch:

After Batch 1:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ID     ‚îÇ      Content        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ chunk_0  ‚îÇ PDF page 1 content  ‚îÇ
‚îÇ chunk_1  ‚îÇ PDF page 2 content  ‚îÇ
‚îÇ chunk_2  ‚îÇ PDF page 3 content  ‚îÇ
‚îÇ   ...    ‚îÇ        ...          ‚îÇ
‚îÇ chunk_19 ‚îÇ Video chunk 11      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Total: 20 chunks ‚úì

After Batch 2 (OVERWRITES!):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ID     ‚îÇ      Content        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ chunk_0  ‚îÇ Video chunk 12 ‚ùå   ‚îÇ  ‚Üê Overwrote PDF page 1!
‚îÇ chunk_1  ‚îÇ Video chunk 13 ‚ùå   ‚îÇ  ‚Üê Overwrote PDF page 2!
‚îÇ chunk_2  ‚îÇ Video chunk 14 ‚ùå   ‚îÇ  ‚Üê Overwrote PDF page 3!
‚îÇ   ...    ‚îÇ        ...          ‚îÇ
‚îÇ chunk_19 ‚îÇ Video chunk 31 ‚ùå   ‚îÇ  ‚Üê Overwrote Video chunk 11!
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Total: 20 chunks (still 20, not 40!) ‚ö†Ô∏è

After Batch 3 (OVERWRITES AGAIN!):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ID     ‚îÇ      Content        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ chunk_0  ‚îÇ Video chunk 32 ‚ùå   ‚îÇ  ‚Üê Overwrote Video chunk 12!
‚îÇ chunk_1  ‚îÇ Video chunk 33 ‚ùå   ‚îÇ  ‚Üê Overwrote Video chunk 13!
‚îÇ chunk_2  ‚îÇ Video chunk 34 ‚ùå   ‚îÇ  ‚Üê Overwrote Video chunk 14!
‚îÇ chunk_3  ‚îÇ Video chunk 15      ‚îÇ
‚îÇ   ...    ‚îÇ        ...          ‚îÇ
‚îÇ chunk_19 ‚îÇ Video chunk 31      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Total: 20 chunks (STILL 20!) ‚ùå

Result: Only last 20 chunks survive, first 23 chunks LOST!
```

---

## The Fix

### Solution 1 (Attempted): Global Counter

**First attempt:**

```python
# This was my first idea - maintain a global counter
chunk_counter = 0

def embed_and_store_chunks(chunks: List[Dict], collection: Collection) -> None:
    global chunk_counter  # ‚ö†Ô∏è Using global state - not ideal

    for chunk in chunks:
        ids_to_add.append(f"chunk_{chunk_counter}")
        chunk_counter += 1
```

**Problems:**
- ‚ùå Global mutable state (bad practice)
- ‚ùå Not thread-safe
- ‚ùå Requires careful reset between ingestion runs
- ‚ùå Breaks if batches are processed out of order

### Solution 2 (Better): Pass Starting Index

```python
def embed_and_store_chunks(
    chunks: List[Dict],
    collection: Collection,
    start_index: int = 0  # ‚Üê Pass the starting index
) -> None:

    for i, chunk in enumerate(chunks):
        chunk_id = start_index + i  # ‚Üê Add offset
        ids_to_add.append(f"chunk_{chunk_id}")

# Caller:
batch_start = 0
for i in range(0, len(chunks), batch_size):
    batch = chunks[i:i+batch_size]
    embed_and_store_chunks(batch, collection, start_index=batch_start)
    batch_start += len(batch)
```

**Better, but:**
- ‚ùå Still order-dependent
- ‚ùå Re-running ingestion creates different IDs
- ‚ùå Can't detect if same content ingested twice

### Solution 3 (BEST): Content-Based Hashing ‚úÖ

**Final implementation:**

```python
import hashlib

def embed_and_store_chunks(chunks: List[Dict[str, str]], collection: Collection) -> None:
    """
    Generates embeddings for text chunks and stores them in ChromaDB.
    """
    if not chunks:
        print("No chunks to embed. Skipping.")
        return

    print(f"Embedding and storing {len(chunks)} chunks...")

    # Initialize Azure OpenAI client
    client = AzureOpenAI(...)

    # Prepare data for ChromaDB
    documents_to_add = []
    metadatas_to_add = []
    ids_to_add = []

    # ‚úÖ FIXED VERSION
    for chunk in chunks:  # No enumerate!
        documents_to_add.append(chunk["content"])
        metadatas_to_add.append({"source": chunk["source"]})

        # Generate globally unique ID based on content hash
        # Format: source + content ‚Üí SHA256 ‚Üí first 16 chars
        unique_string = f"{chunk['source']}_{chunk['content']}"
        chunk_hash = hashlib.sha256(unique_string.encode()).hexdigest()[:16]
        ids_to_add.append(f"chunk_{chunk_hash}")  # ‚úÖ GLOBALLY UNIQUE

    # Generate embeddings and store
    response = client.embeddings.create(input=documents_to_add, model=...)
    embeddings = [item.embedding for item in response.data]

    collection.add(
        embeddings=embeddings,
        documents=documents_to_add,
        metadatas=metadatas_to_add,
        ids=ids_to_add  # ‚úÖ IDs like: chunk_a3f8b2c1d4e5f6a7
    )

    print(f"‚úì Successfully embedded and stored {len(chunks)} chunks")
```

### Why This Works

**Properties of Content-Based Hashing:**

1. **Globally Unique**
   ```python
   chunk_1 = {"source": "file.pdf", "content": "text A"}
   chunk_2 = {"source": "file.pdf", "content": "text B"}

   hash_1 = SHA256("file.pdf_text A") = "a3f8b2c1d4e5f6a7..."
   hash_2 = SHA256("file.pdf_text B") = "7b9c8a3e2f1d0c4b..."
   # Different content ‚Üí different hash ‚úì
   ```

2. **Deterministic**
   ```python
   # Same content always produces same ID
   hash_1 = SHA256("file.pdf_text A") = "a3f8b2c1d4e5f6a7"
   hash_2 = SHA256("file.pdf_text A") = "a3f8b2c1d4e5f6a7"  # ‚úì Same!
   ```

3. **Collision-Resistant**
   ```python
   # SHA256 has 2^256 possible values
   # Probability of collision: ~0% for our use case
   ```

4. **Batch-Independent**
   ```python
   # Processing order doesn't matter
   Batch 1: chunk_a3f8b2c1, chunk_7b9c8a3e
   Batch 2: chunk_2f1d0c4b, chunk_e5f6a789
   # No conflicts regardless of order ‚úì
   ```

---

## Verification

### After Applying the Fix

```bash
$ python3 ingest_all.py

================================================================================
COMPLETE DATA INGESTION
================================================================================

üìÑ Processing PDF with Document Intelligence...
‚úÖ PDF processed: 9151 characters

üé¨ Processing video...
‚úÖ Video processed: 23184 characters

================================================================================
CHUNKING & EMBEDDING
================================================================================

üìù Chunking 2 documents...
‚úÖ Created 43 chunks

üóÑÔ∏è  Storing in database...
   Processing batch 1/3 (20 chunks)...
‚úì Successfully embedded and stored 20 chunks in vector database
   Processing batch 2/3 (20 chunks)...
‚úì Successfully embedded and stored 20 chunks in vector database
   Processing batch 3/3 (3 chunks)...
‚úì Successfully embedded and stored 3 chunks in vector database

================================================================================
‚úÖ SUCCESS! Database has 43 chunks  # ‚úÖ CORRECT NOW!
================================================================================
```

### Check Chunk Count

```bash
$ python3 -c "from src.vector_store import get_vector_database_collection; print(f'Chunks: {get_vector_database_collection().count()}')"

Chunks: 43  # ‚úÖ Perfect!
```

### Inspect IDs

```python
collection = get_vector_database_collection()
results = collection.get()
ids = results['ids']

print(f"Total chunks: {len(ids)}")
print(f"\nSample IDs:")
for id in sorted(ids)[:5]:
    print(f"  {id}")
```

**Output:**

```bash
Total chunks: 43  # ‚úÖ All chunks present!

Sample IDs:
  chunk_022a6e9389f67234  # ‚úÖ Content-based hash
  chunk_0a098ad629adc167  # ‚úÖ Content-based hash
  chunk_0c7f3b9a8e2d1f45  # ‚úÖ Content-based hash
  chunk_1d8e4a7c9b3f2e56  # ‚úÖ Content-based hash
  chunk_2f9a5b8c7d4e3a61  # ‚úÖ Content-based hash
```

### Verify Source Distribution

```bash
$ python3 verify_data_integrity.py

Chunks per source:
  ‚Ä¢ RagPresenetation.pdf: 14 chunks  # ‚úÖ Correct (was 3)
  ‚Ä¢ database-for-genAI.mp4: 29 chunks  # ‚úÖ Correct (was 17)

‚úÖ ALL CHECKS PASSED - Data integrity verified!
```

---

## Code Comparison (Side-by-Side)

### Before (BUGGY)
```python
for i, chunk in enumerate(chunks):
    documents_to_add.append(chunk["content"])
    metadatas_to_add.append({"source": chunk["source"]})
    ids_to_add.append(f"chunk_{i}")
    # ‚ùå Problem: i resets in each batch
    # ‚ùå Result: ID collisions and overwrites
```

### After (FIXED)
```python
import hashlib

for chunk in chunks:
    documents_to_add.append(chunk["content"])
    metadatas_to_add.append({"source": chunk["source"]})

    # Generate unique ID from content
    unique_string = f"{chunk['source']}_{chunk['content']}"
    chunk_hash = hashlib.sha256(unique_string.encode()).hexdigest()[:16]
    ids_to_add.append(f"chunk_{chunk_hash}")
    # ‚úÖ Solution: Content-based hashing
    # ‚úÖ Result: Globally unique IDs, no collisions
```

---

## Lessons Learned

### 1. Silent Failures Are the Worst

**The danger:**
- ‚úÖ All operations succeeded
- ‚úÖ No error messages
- ‚ùå Data was silently lost

**Prevention:**
- Always verify results after operations
- Don't trust success messages alone
- Implement data integrity checks

### 2. enumerate() in Batch Processing

**The trap:**
```python
for i in range(0, 100, 20):  # Process in batches of 20
    batch = items[i:i+20]
    for j, item in enumerate(batch):  # ‚ùå j resets to 0 each iteration!
        print(f"Item {j}")  # Always prints 0-19, never 20-99!
```

**Better approaches:**
```python
# Option 1: Use absolute index
for i in range(0, 100, 20):
    batch = items[i:i+20]
    for j, item in enumerate(batch):
        absolute_index = i + j  # ‚úì Correct: 0-99
        print(f"Item {absolute_index}")

# Option 2: Use content-based IDs (best for databases)
for i in range(0, 100, 20):
    batch = items[i:i+20]
    for item in batch:
        item_id = hash(item.content)  # ‚úì Independent of order
```

### 3. ChromaDB Upsert Behavior

**What we learned:**
- ChromaDB's `add()` method is actually "upsert" (update or insert)
- Duplicate IDs ‚Üí silent overwrites (no error)
- This is intentional design for updating documents
- Must ensure IDs are globally unique

**From ChromaDB docs:**
```python
collection.add(
    documents=["doc1", "doc2"],
    ids=["id1", "id1"]  # ‚ö†Ô∏è Duplicate ID!
)
# Result: Only 1 document stored (id1 with doc2)
# No error thrown!
```

### 4. Content-Based IDs Are Superior

**Benefits:**
- ‚úÖ Globally unique (no collisions)
- ‚úÖ Deterministic (same content ‚Üí same ID)
- ‚úÖ Idempotent (can re-run ingestion safely)
- ‚úÖ Order-independent (batch processing safe)
- ‚úÖ Detects duplicates automatically

**Use cases:**
- Vector databases (ChromaDB, Pinecone, Qdrant)
- Distributed systems (Kafka, event sourcing)
- Content-addressable storage (IPFS, Git)
- Caching systems

### 5. Testing Requirements

**What unit tests would have missed:**
- Sequential ID generation looks correct in isolation
- Single-batch tests would pass
- Only multi-batch scenarios reveal the bug

**What caught it:**
- Real integration test with full dataset
- Data verification after ingestion
- Checking actual chunk count vs. expected

**Takeaway:** Real end-to-end tests with production data volumes are essential!

---

## Summary

### The Bug
```python
# Used enumerate() which resets in each batch
for i, chunk in enumerate(chunks):
    ids.append(f"chunk_{i}")  # ‚ùå IDs 0-19 created in EVERY batch
```

### The Impact
- 43 chunks ingested ‚Üí Only 20 stored (53% data loss!)
- No error messages (silent failure)
- Later batches overwrote earlier batches

### The Fix
```python
# Use content-based SHA256 hashing
for chunk in chunks:
    unique_string = f"{chunk['source']}_{chunk['content']}"
    chunk_hash = hashlib.sha256(unique_string.encode()).hexdigest()[:16]
    ids.append(f"chunk_{chunk_hash}")  # ‚úÖ Globally unique IDs
```

### The Result
- ‚úÖ All 43 chunks stored correctly
- ‚úÖ No ID collisions
- ‚úÖ Idempotent (can re-run safely)
- ‚úÖ Production-ready

---

**This was a production-grade bug that taught valuable lessons about data integrity, batch processing, and the importance of comprehensive testing!**
