Of course. Here is the complete and consolidated task4.md file. It merges the original audio transcription task with the new MP4 handling capability into a single, comprehensive spec.

Spec Task DATA-4: Audio and Video Transcription Client
üéØ Objective
To implement a robust data loading system that can transcribe audio from both direct audio files (e.g., .mp3, .wav) and video files (.mp4) by first extracting the audio stream. This function must be securely configured and fully testable without making live network requests.

üîë Key Components & Rationale
openai library: The official Python client for interacting with Azure OpenAI's Whisper model for transcription.

FFmpeg & ffmpeg-python library: FFmpeg is the industry-standard tool for multimedia processing. The Python library provides a clean interface to run FFmpeg commands to extract audio from video files.

src/config.py: Reuses the settings object to securely provide credentials to the Azure OpenAI client.

Test Fixtures: We'll use both sample.wav and sample.mp4 files in /tests/fixtures to test both direct audio and video-to-audio pipelines.

Mocking with pytest-mock: Crucial for testing. We will mock both the external FFmpeg calls and the Azure OpenAI API calls to ensure our tests are fast, reliable, and free of external dependencies.

‚úÖ Acceptance Criteria
The openai and ffmpeg-python libraries are added to requirements.txt.

Test fixtures sample.wav and sample.mp4 exist in /tests/fixtures.

The core transcribe_audio_file function exists and is unit-tested with a mock.

The load_from_directory function is updated to handle .pdf, audio, and .mp4 files.

A new test, test_load_from_directory_handles_mp4, is added to verify the MP4 processing logic.

The Dockerfile is updated to include the FFmpeg system dependency.

üìù Detailed Steps
1. Update Dependencies üêç

Add the following lines to your requirements.txt file:

# AI Services
openai

# Multimedia Processing
ffmpeg-python
Install the libraries:

Bash

pip install -r requirements.txt
2. External Prerequisite: Install FFmpeg

For local development, you need the FFmpeg program installed on your system.

macOS: brew install ffmpeg

Ubuntu/Debian: sudo apt update && sudo apt install ffmpeg

Windows: Download from the official site and add it to your system's PATH.

(This will be handled in the Dockerfile for containerized deployment).

3. Create Test Fixtures

Audio: Record a short audio clip saying "This is a test transcription." and save it as sample.wav in the tests/fixtures directory.

Video: Create a short (2-3 second), silent video file and save it as sample.mp4 in the tests/fixtures directory.

4. Implement Core Transcription Function

Append the following function to src/data_loader.py. This function handles the direct transcription of audio files.

Python

# src/data_loader.py (append to existing file)
from openai import AzureOpenAI
from src.config import settings
# ... (keep other imports)

def transcribe_audio_file(file_path: str | Path) -> str:
    """
    Transcribes an audio file using Azure OpenAI's Whisper model.
    """
    # ... (Implementation from the original task)
5. Modify the Directory Loader for Video

Update load_from_directory in src/data_loader.py to handle MP4 files by extracting their audio.

Python

# src/data_loader.py (add new imports and update function)
import ffmpeg
import tempfile

# ...

def load_from_directory(directory_path: str | Path) -> list[dict]:
    if not isinstance(directory_path, Path):
        directory_path = Path(directory_path)

    documents = []
    supported_audio = {".wav", ".mp3", ".m4a"}
    supported_video = {".mp4"}

    for file_path in directory_path.iterdir():
        if not file_path.is_file():
            continue

        doc = None
        if file_path.suffix.lower() == ".pdf":
            content = load_text_from_pdf(file_path)
            if content: doc = {"source": file_path.name, "content": content}
        elif file_path.suffix.lower() in supported_audio:
            content = transcribe_audio_file(file_path)
            if content: doc = {"source": file_path.name, "content": content}
        elif file_path.suffix.lower() in supported_video:
            with tempfile.NamedTemporaryFile(suffix=".mp3", delete=True) as tmp_audio_file:
                try:
                    print(f"Extracting audio from {file_path.name}...")
                    ffmpeg.input(str(file_path)).output(tmp_audio_file.name, acodec='libmp3lame').run(overwrite_output=True, quiet=True)
                    content = transcribe_audio_file(tmp_audio_file.name)
                    if content: doc = {"source": file_path.name, "content": content}
                except ffmpeg.Error as e:
                    print(f"Error extracting audio from {file_path.name}: {e.stderr.decode()}")

        if doc:
            documents.append(doc)

    return documents
6. Update Dockerfile for FFmpeg üê≥

To ensure FFmpeg is available inside your Docker container, add the installation command to your Dockerfile.

Dockerfile

# Use an official Python runtime as a parent image
FROM python:3.11-slim

# Install system dependencies, including ffmpeg
RUN apt-get update && apt-get install -y ffmpeg

# ... (rest of Dockerfile)
7. Implement Unit Tests üß™

Add the following tests to tests/test_data_loader.py.

Python

# tests/test_data_loader.py (append to existing file)
from unittest.mock import MagicMock

# Test for the core transcription function
def test_transcribe_audio_file_mocked(mocker):
    """
    Tests the audio transcription function with a mocked AzureOpenAI client.
    """
    # ... (Implementation from the original task)

# Test for the MP4 handling in the directory loader
def test_load_from_directory_handles_mp4(mocker):
    """
    Tests that the directory loader correctly handles an MP4 file
    by mocking the audio extraction and transcription process.
    """
    fixture_dir = Path(__file__).parent / "fixtures"

    mock_ffmpeg = mocker.patch("src.data_loader.ffmpeg.run")
    mock_transcriber = mocker.patch(
        "src.data_loader.transcribe_audio_file",
        return_value="Text from video."
    )
    # Mock the PDF loader to ignore the PDF in the fixtures dir for this test
    mocker.patch("src.data_loader.load_text_from_pdf", return_value=None)

    documents = load_from_directory(fixture_dir)
    mp4_doc = next((doc for doc in documents if doc["source"] == "sample.mp4"), None)

    assert mp4_doc is not None, "MP4 document was not processed"
    assert mp4_doc["content"] == "Text from video."
    mock_ffmpeg.assert_called_once()
    mock_transcriber.assert_called_once()
üß™ TDD - Verification
Navigate to the root rag-chatbot directory.

Run the test suite:

Bash

pytest
The output should show all tests passing, confirming that your code correctly handles both direct audio files and the logic for processing video files.

For end-to-end verification, run the application via Docker and place both a .wav and an .mp4 file in the /data directory to confirm both are ingested.